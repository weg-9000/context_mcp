import asyncio
import json
import time
import hashlib
import re
from typing import Dict, List, Any, Optional, Union, AsyncGenerator, TypedDict
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from urllib.parse import quote
import logging

# HTTP í´ë¼ì´ì–¸íŠ¸
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    AIOHTTP_AVAILABLE = False
    print("aiohttp not available, web search will be limited")

# LangChain ê´€ë ¨ imports
try:
    from langchain.agents import AgentExecutor, create_react_agent
    from langchain.agents import Tool as LangChainTool
    from langchain.prompts import PromptTemplate
    from langchain.schema import BaseMessage, HumanMessage, SystemMessage
    from langchain_openai import ChatOpenAI
    from langchain.callbacks.manager import CallbackManagerForToolRun
    from langchain.tools import BaseTool
    # LangGraph imports
    from langgraph.graph import StateGraph, END
    from langgraph.prebuilt import ToolExecutor
    from langgraph.checkpoint import MemorySaver
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False
    print("LangChain/LangGraph not available, using fallback implementation")

# ì¶”ê°€ëœ LangChain ì˜ˆì™¸ íƒ€ì…ë“¤
from langchain.schema import LangChainException
from langchain.tools.base import ToolException


from modular_agent_architecture import (
    ModularAgent, AgentConfig, ProcessingRequest, ProcessingResponse,
    FrameworkAdapter, AgentCapability, AgentFramework
)

# ê¸°ì¡´ Enum í´ë˜ìŠ¤ë“¤ ìœ ì§€
class RetrievalMode(Enum):
    """ê²€ìƒ‰ ëª¨ë“œ"""
    COMPREHENSIVE = "comprehensive"  # í¬ê´„ì  ê²€ìƒ‰
    FOCUSED = "focused"  # ì§‘ì¤‘ ê²€ìƒ‰
    REAL_TIME = "real_time"  # ì‹¤ì‹œê°„ ê²€ìƒ‰
    MULTIMODAL = "multimodal"  # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰

class DataSource(Enum):
    """ë°ì´í„° ì†ŒìŠ¤ ìœ í˜•"""
    WEB_SEARCH = "web_search"
    API_ENDPOINT = "api_endpoint"
    DATABASE = "database"
    FILE_SYSTEM = "file_system"
    SOCIAL_MEDIA = "social_media"
    NEWS_FEED = "news_feed"
    ACADEMIC = "academic"
    MULTIMEDIA = "multimedia"

# ê¸°ì¡´ dataclassë“¤ ìœ ì§€
@dataclass
class RetrievalTask:
    """ê²€ìƒ‰ ì‘ì—… ì •ì˜"""
    task_id: str
    query: str
    data_sources: List[DataSource]
    priority: int
    max_results: int
    freshness_requirement: float  # 0.0 ~ 1.0
    relevance_threshold: float
    language_preference: str = "auto"
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class RetrievedItem:
    """ê²€ìƒ‰ëœ í•­ëª©"""
    item_id: str
    source: DataSource
    content: Any
    title: Optional[str] = None
    url: Optional[str] = None
    timestamp: float = field(default_factory=time.time)
    relevance_score: float = 0.0
    freshness_score: float = 0.0
    quality_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)

# SearchCache í´ë˜ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
class SearchCache:
    """ê²€ìƒ‰ ê²°ê³¼ ìºì‹± ì‹œìŠ¤í…œ"""
    def __init__(self, ttl: int = 3600, max_size: int = 1000):
        self._cache = {}
        self._ttl = ttl
        self._max_size = max_size
        self._access_times = {}

    def _get_cache_key(self, query: str, params: Dict = None) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        cache_data = f"{query}_{json.dumps(params or {}, sort_keys=True)}"
        return hashlib.md5(cache_data.encode()).hexdigest()

    def get(self, query: str, params: Dict = None) -> Optional[str]:
        """ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        key = self._get_cache_key(query, params)
        if key in self._cache:
            result, timestamp = self._cache[key]
            if time.time() - timestamp < self._ttl:
                self._access_times[key] = time.time()
                return result
            else:
                self._remove_cache_entry(key)
        return None

    def set(self, query: str, result: str, params: Dict = None):
        """ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        if len(self._cache) >= self._max_size:
            self._evict_oldest()
        key = self._get_cache_key(query, params)
        self._cache[key] = (result, time.time())
        self._access_times[key] = time.time()

    def _remove_cache_entry(self, key: str):
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        if key in self._cache:
            del self._cache[key]
        if key in self._access_times:
            del self._access_times[key]

    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±° (LRU)"""
        if not self._access_times:
            return
        oldest_key = min(self._access_times.keys(), key=lambda k: self._access_times[k])
        self._remove_cache_entry(oldest_key)

    def clear_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self._cache.items()
            if current_time - timestamp >= self._ttl
        ]
        for key in expired_keys:
            self._remove_cache_entry(key)

    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        return {
            "total_entries": len(self._cache),
            "max_size": self._max_size,
            "ttl_seconds": self._ttl,
            "memory_usage_mb": len(str(self._cache)) / (1024 * 1024)
        }

# LangGraph State ì •ì˜
class AgentState(TypedDict):
    """LangGraph ìƒíƒœ ì •ì˜"""
    messages: List[BaseMessage]
    query: str
    results: Dict[str, Any]
    current_step: str
    metadata: Dict[str, Any]

# LangChain ë„êµ¬ë“¤
class EnhancedWebSearchTool(BaseTool):
    """í–¥ìƒëœ ì›¹ ê²€ìƒ‰ ë„êµ¬ (LangChain)"""
    name = "enhanced_web_search"
    description = "DuckDuckGo APIë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"

    def __init__(self, http_session: Optional[aiohttp.ClientSession] = None):
        super().__init__()
        self.http_session = http_session
        self.logger = logging.getLogger(f"{__name__}.EnhancedWebSearchTool")

    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """ë™ê¸° ì‹¤í–‰ (ë¹„ê¶Œì¥)"""
        return asyncio.run(self._arun(query, run_manager))

    async def _arun(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """ë¹„ë™ê¸° ì›¹ ê²€ìƒ‰ ì‹¤í–‰"""
        if not self.http_session:
            return await self._fallback_web_search(query)

        try:
            search_url = f"https://api.duckduckgo.com/?q={quote(query)}&format=json&no_html=1&skip_disambig=1"
            async with self.http_session.get(search_url) as response:
                if response.status == 200:
                    data = await response.json()
                    results = self._parse_search_results(data)
                    return results if results else await self._fallback_web_search(query)
                else:
                    return await self._fallback_web_search(query)
        except ToolException as e:
            self.logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return await self._fallback_web_search(query)
        except aiohttp.ClientError as e:
            self.logger.error(f"HTTP í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {e}")
            return await self._fallback_web_search(query)
        except LangChainException as e:
            self.logger.error(f"LangChain ì˜¤ë¥˜: {e}")
            return await self._fallback_web_search(query)
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return await self._fallback_web_search(query)

    def _parse_search_results(self, data: Dict[str, Any]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±"""
        results = []
        if data.get("Abstract"):
            results.append(f"**ê°œìš”**: {data['Abstract']}")
        if data.get("Answer"):
            results.append(f"**ì§ì ‘ ë‹µë³€**: {data['Answer']}")
        if data.get("RelatedTopics"):
            results.append("**ê´€ë ¨ ì£¼ì œ**:")
            for i, topic in enumerate(data["RelatedTopics"][:3], 1):
                if isinstance(topic, dict) and topic.get("Text"):
                    text = topic["Text"][:200] + "..." if len(topic["Text"]) > 200 else topic["Text"]
                    results.append(f"{i}. {text}")
        return "\n\n".join(results) if results else ""

    async def _fallback_web_search(self, query: str) -> str:
        """í´ë°± ì›¹ ê²€ìƒ‰"""
        return f"""
**ê²€ìƒ‰ì–´**: {query}
**ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼**:
1. {query} ê°œìš” ë° ê¸°ë³¸ ì •ë³´
2. {query} ì‚¬ìš©ë²• ë° ì˜ˆì œ
3. {query} ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
4. {query} ë¬¸ì œ í•´ê²° ê°€ì´ë“œ
5. {query} ìµœì‹  ë™í–¥

**ì‹ ë¢°ë„**: ì¤‘ê°„ (ì‹œë®¬ë ˆì´ì…˜)
"""

class APIRetrieverTool(BaseTool):
    """API ê²€ìƒ‰ ë„êµ¬ (LangChain)"""
    name = "api_retriever"
    description = "ë‹¤ì–‘í•œ API ì—”ë“œí¬ì¸íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"

    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """API ê²€ìƒ‰ ì‹¤í–‰"""
        return f"""
**ê²€ìƒ‰ì–´**: {query}
**ë°œê²¬ëœ API ë°ì´í„°**:
- GitHub API: ê´€ë ¨ ë¦¬í¬ì§€í† ë¦¬ {3}ê°œ ë°œê²¬
- Stack Overflow API: ê´€ë ¨ ì§ˆë¬¸ {5}ê°œ ë°œê²¬
- Documentation API: ê³µì‹ ë¬¸ì„œ {2}ê°œ ë°œê²¬

**ë°ì´í„° í’ˆì§ˆ**: ë†’ìŒ (API ê¸°ë°˜ êµ¬ì¡°í™”ëœ ë°ì´í„°)
"""

    async def _arun(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        return self._run(query, run_manager)

class MultimediaProcessorTool(BaseTool):
    """ë©€í‹°ë¯¸ë””ì–´ ì²˜ë¦¬ ë„êµ¬ (LangChain)"""
    name = "multimedia_processor"
    description = "ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ì½˜í…ì¸ ë¥¼ ì²˜ë¦¬í•˜ê³  ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"

    def _run(self, content: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """ë©€í‹°ë¯¸ë””ì–´ ì²˜ë¦¬"""
        return f"""
# ë©€í‹°ë¯¸ë””ì–´ ì²˜ë¦¬ ê²°ê³¼

**ì²˜ë¦¬ ëŒ€ìƒ**: {content}
**ì²˜ë¦¬ëœ ì½˜í…ì¸  ìœ í˜•**:
- ì´ë¯¸ì§€: ìŠ¤í¬ë¦°ìƒ· ë° ë‹¤ì´ì–´ê·¸ë¨ ë¶„ì„
- ë¹„ë””ì˜¤: íŠœí† ë¦¬ì–¼ ë° ë°ëª¨ ì˜ìƒ ì „ì‚¬
- ì˜¤ë””ì˜¤: íŒŸìºìŠ¤íŠ¸ ë° ê°•ì˜ ìŒì„± ì¸ì‹

**ì¶”ì¶œëœ ì •ë³´ í’ˆì§ˆ**: ì¤‘ìƒ
"""

    async def _arun(self, content: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        return self._run(content, run_manager)

class RelevanceEvaluatorTool(BaseTool):
    """ê´€ë ¨ì„± í‰ê°€ ë„êµ¬ (LangChain)"""
    name = "relevance_evaluator"
    description = "ê²€ìƒ‰ëœ ì½˜í…ì¸ ì™€ ì¿¼ë¦¬ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤"

    def _run(self, content: str, query: str = "", run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        """ê´€ë ¨ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ í‰ê°€
        query_words = set(query.lower().split()) if query else set()
        content_words = set(content.lower().split())
        if query_words:
            overlap = len(query_words.intersection(content_words))
            relevance_score = min(overlap / len(query_words), 1.0)
        else:
            relevance_score = 0.8

        return f"""
# ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼

**í‰ê°€ ì ìˆ˜**: {relevance_score:.2f} / 1.0
**ê¶Œì¥ì‚¬í•­**: {'í¬í•¨' if relevance_score > 0.7 else 'ê²€í†  í•„ìš”'}
"""

    async def _arun(self, content: str, query: str = "", run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        return self._run(content, query, run_manager)

class LangChainAdapter(FrameworkAdapter):
    """LangChain í”„ë ˆì„ì›Œí¬ ì–´ëŒ‘í„° (CrewAI ëŒ€ì²´)"""

    def __init__(self):
        self.workflow: Optional[StateGraph] = None
        self.agents: Dict[str, AgentExecutor] = {}
        self.tools: Dict[str, BaseTool] = {}
        self.llm = None
        self.is_initialized = False
        self._search_cache = SearchCache()
        self._http_session: Optional[aiohttp.ClientSession] = None
        self._semaphore = asyncio.Semaphore(5)  # ë™ì‹œ ê²€ìƒ‰ ì œí•œ

        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"{__name__}.LangChainAdapter")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.search_metrics = {
            "total_searches": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "failed_searches": 0,
            "average_response_time": 0.0
        }

    def get_framework_name(self) -> str:
        return "LangChain"

    async def initialize(self, config: Dict[str, Any]) -> bool:
        """LangChain ì´ˆê¸°í™”"""
        try:
            # HTTP ì„¸ì…˜ ì´ˆê¸°í™”
            if AIOHTTP_AVAILABLE:
                timeout = aiohttp.ClientTimeout(total=config.get("request_timeout", 30))
                self._http_session = aiohttp.ClientSession(timeout=timeout)

            if not LANGCHAIN_AVAILABLE:
                self.logger.info("LangChain ì‚¬ìš© ë¶ˆê°€, í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™”")
                return await self._initialize_fallback(config)

            # LLM ì„¤ì •
            try:
                self.llm = ChatOpenAI(
                    model=config.get("llm_model", "gpt-3.5-turbo"),
                    temperature=config.get("temperature", 0.1),
                    request_timeout=config.get("llm_timeout", 30)
                )
            except ValueError as e:
                # API í‚¤, ëª¨ë¸ëª… ë“± ì„¤ì • ì˜¤ë¥˜
                self.logger.warning(f"OpenAI ì„¤ì • ì˜¤ë¥˜: {e}, í´ë°± ëª¨ë“œ ì‚¬ìš©")
                self.llm = None
            except Exception as e:
                # OpenAI API ê´€ë ¨ ëª¨ë“  ì˜¤ë¥˜ (ë„¤íŠ¸ì›Œí¬, ì¸ì¦, í• ë‹¹ëŸ‰ ë“±)
                self.logger.warning(f"OpenAI API ì˜¤ë¥˜: {e}, í´ë°± ëª¨ë“œ ì‚¬ìš©")
                self.llm = None
            except LangChainException as e:
                self.logger.warning(f"LangChain ì˜¤ë¥˜: {e}, í´ë°± ëª¨ë“œ ì‚¬ìš©")
                self.llm = None

            # ë„êµ¬ ì´ˆê¸°í™”
            await self._initialize_tools(config)

            # ì—ì´ì „íŠ¸ë“¤ ìƒì„±
            if self.llm:
                await self._create_specialist_agents(config)
                self._build_workflow()

            self.is_initialized = True
            self.logger.info("LangChain ì–´ëŒ‘í„° ì´ˆê¸°í™” ì™„ë£Œ")
            return True

        except ValueError as e:
            # ì˜ëª»ëœ ì…ë ¥ê°’ì´ë‚˜ ì„¤ì • ì˜¤ë¥˜
            self.logger.error(f"ì—ì´ì „íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)
        except LangChainException as e:
            # LangChain í”„ë ˆì„ì›Œí¬ ê´€ë ¨ ì˜¤ë¥˜
            self.logger.error(f"LangChain ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)
        except Exception as e:
            # ê¸°íƒ€ ì˜ˆìƒì¹˜ ëª»í•œ ì—ì´ì „íŠ¸ ê´€ë ¨ ì˜¤ë¥˜
            self.logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)
        except LangChainException as e:
            self.logger.error(f"LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return await self._initialize_fallback(config)
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)

    async def _initialize_fallback(self, config: Dict[str, Any]) -> bool:
        """í´ë°± ì´ˆê¸°í™”"""
        self.logger.info("í´ë°± ëª¨ë“œ ì´ˆê¸°í™” ì¤‘...")
        # ìºì‹œ ì„¤ì •
        cache_ttl = config.get("cache_ttl", 3600)
        cache_size = config.get("cache_size", 1000)
        self._search_cache = SearchCache(ttl=cache_ttl, max_size=cache_size)

        # HTTP ì„¸ì…˜ ì´ˆê¸°í™”
        if AIOHTTP_AVAILABLE and not self._http_session:
            timeout = aiohttp.ClientTimeout(total=config.get("request_timeout", 30))
            self._http_session = aiohttp.ClientSession(timeout=timeout)

        self.is_initialized = True
        self.logger.info("í´ë°± ì´ˆê¸°í™” ì™„ë£Œ")
        return True

    async def _initialize_tools(self, config: Dict[str, Any]):
        """ë„êµ¬ ì´ˆê¸°í™”"""
        if not LANGCHAIN_AVAILABLE:
            return

        try:
            # ì›¹ ê²€ìƒ‰ ë„êµ¬
            self.tools["web_search"] = EnhancedWebSearchTool(http_session=self._http_session)
            # API ê²€ìƒ‰ ë„êµ¬
            self.tools["api_retriever"] = APIRetrieverTool()
            # ë©€í‹°ë¯¸ë””ì–´ ì²˜ë¦¬ ë„êµ¬
            self.tools["multimedia_processor"] = MultimediaProcessorTool()
            # ê´€ë ¨ì„± í‰ê°€ ë„êµ¬
            self.tools["relevance_evaluator"] = RelevanceEvaluatorTool()

            self.logger.info(f"ë„êµ¬ {len(self.tools)}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")

        except ToolException as e:
            self.logger.error(f"ë„êµ¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        except LangChainException as e:
            self.logger.error(f"LangChain ë„êµ¬ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ë„êµ¬ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    async def _create_specialist_agents(self, config: Dict[str, Any]):
        """ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ë“¤ ìƒì„± (LangChain ë²„ì „)"""
        if not LANGCHAIN_AVAILABLE or not self.llm:
            return

        try:
            # ì›¹ ê²€ìƒ‰ ì „ë¬¸ê°€
            web_tools = [self.tools["web_search"], self.tools["relevance_evaluator"]]
            web_prompt = PromptTemplate(
                template="""ë‹¹ì‹ ì€ ì›¹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ ì›¹ì—ì„œ ìµœì‹ ì˜ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
                input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
            )

            web_agent = create_react_agent(self.llm, web_tools, web_prompt)
            self.agents["web_specialist"] = AgentExecutor(
                agent=web_agent,
                tools=web_tools,
                verbose=config.get("verbose", False),
                return_intermediate_steps=True
            )

            # API ë°ì´í„° ì „ë¬¸ê°€
            api_tools = [self.tools["api_retriever"]]
            api_prompt = PromptTemplate(
                template="""ë‹¹ì‹ ì€ API ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ì–‘í•œ APIë¥¼ í†µí•´ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
                input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
            )

            api_agent = create_react_agent(self.llm, api_tools, api_prompt)
            self.agents["api_specialist"] = AgentExecutor(
                agent=api_agent,
                tools=api_tools,
                verbose=config.get("verbose", False),
                return_intermediate_steps=True
            )

            # ë©€í‹°ë¯¸ë””ì–´ ì „ë¬¸ê°€
            multimedia_tools = [self.tools["multimedia_processor"]]
            multimedia_prompt = PromptTemplate(
                template="""ë‹¹ì‹ ì€ ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ì½˜í…ì¸ ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
                input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
            )

            multimedia_agent = create_react_agent(self.llm, multimedia_tools, multimedia_prompt)
            self.agents["multimedia_specialist"] = AgentExecutor(
                agent=multimedia_agent,
                tools=multimedia_tools,
                verbose=config.get("verbose", False),
                return_intermediate_steps=True
            )

            # í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€
            quality_tools = [self.tools["relevance_evaluator"]]
            quality_prompt = PromptTemplate(
                template="""ë‹¹ì‹ ì€ ì •ë³´ í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìˆ˜ì§‘ëœ ì •ë³´ì˜ í’ˆì§ˆ, ê´€ë ¨ì„±, ì‹ ì„ ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
                input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
            )

            quality_agent = create_react_agent(self.llm, quality_tools, quality_prompt)
            self.agents["quality_specialist"] = AgentExecutor(
                agent=quality_agent,
                tools=quality_tools,
                verbose=config.get("verbose", False),
                return_intermediate_steps=True
            )

            self.logger.info(f"ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ {len(self.agents)}ê°œ ìƒì„± ì™„ë£Œ")

        except ValueError as e:
            # ì˜ëª»ëœ ì…ë ¥ê°’ì´ë‚˜ ì„¤ì • ì˜¤ë¥˜
            self.logger.error(f"ì—ì´ì „íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)
        except LangChainException as e:
            # LangChain í”„ë ˆì„ì›Œí¬ ê´€ë ¨ ì˜¤ë¥˜
            self.logger.error(f"LangChain ì—ì´ì „íŠ¸ ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)
        except Exception as e:
            # ê¸°íƒ€ ì˜ˆìƒì¹˜ ëª»í•œ ì—ì´ì „íŠ¸ ê´€ë ¨ ì˜¤ë¥˜
            self.logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return await self._initialize_fallback(config)
        except LangChainException as e:
            self.logger.error(f"LangChain ì—ì´ì „íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            self.agents = {}
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì—ì´ì „íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            self.agents = {}

    def _build_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        if not LANGCHAIN_AVAILABLE or not self.agents:
            return

        try:
            # ì›Œí¬í”Œë¡œìš° ìƒì„±
            workflow = StateGraph(AgentState)

            # ë…¸ë“œ ì¶”ê°€
            async def web_search_node(state: AgentState) -> AgentState:
                """ì›¹ ê²€ìƒ‰ ë…¸ë“œ"""
                if "web_specialist" in self.agents:
                    result = await self.agents["web_specialist"].ainvoke({"input": state["query"]})
                    state["results"]["web_search"] = result["output"]
                state["current_step"] = "api_search"
                return state

            async def api_search_node(state: AgentState) -> AgentState:
                """API ê²€ìƒ‰ ë…¸ë“œ"""
                if "api_specialist" in self.agents:
                    result = await self.agents["api_specialist"].ainvoke({"input": state["query"]})
                    state["results"]["api_search"] = result["output"]
                state["current_step"] = "multimedia_process"
                return state

            async def multimedia_node(state: AgentState) -> AgentState:
                """ë©€í‹°ë¯¸ë””ì–´ ì²˜ë¦¬ ë…¸ë“œ"""
                if "multimedia_specialist" in self.agents:
                    result = await self.agents["multimedia_specialist"].ainvoke({"input": state["query"]})
                    state["results"]["multimedia"] = result["output"]
                state["current_step"] = "quality_evaluation"
                return state

            async def quality_node(state: AgentState) -> AgentState:
                """í’ˆì§ˆ í‰ê°€ ë…¸ë“œ"""
                if "quality_specialist" in self.agents:
                    all_results = json.dumps(state["results"], ensure_ascii=False)
                    result = await self.agents["quality_specialist"].ainvoke({"input": all_results})
                    state["results"]["quality_evaluation"] = result["output"]
                state["current_step"] = "complete"
                return state

            # ì›Œí¬í”Œë¡œìš°ì— ë…¸ë“œ ì¶”ê°€
            workflow.add_node("web_search", web_search_node)
            workflow.add_node("api_search", api_search_node)
            workflow.add_node("multimedia_process", multimedia_node)
            workflow.add_node("quality_evaluation", quality_node)

            # ì—£ì§€ ì •ì˜ (ìˆœì°¨ ì‹¤í–‰)
            workflow.add_edge("web_search", "api_search")
            workflow.add_edge("api_search", "multimedia_process")
            workflow.add_edge("multimedia_process", "quality_evaluation")
            workflow.add_edge("quality_evaluation", END)

            # ì‹œì‘ì  ì„¤ì •
            workflow.set_entry_point("web_search")

            # ì»´íŒŒì¼
            self.workflow = workflow.compile()
            self.logger.info("LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ")

        except LangChainException as e:
            self.logger.error(f"ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì‹¤íŒ¨: {e}")
            self.workflow = None
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì˜¤ë¥˜: {e}")
            self.workflow = None

    async def process_request(self, request: ProcessingRequest) -> ProcessingResponse:
        """LangChainì„ ì‚¬ìš©í•œ ìš”ì²­ ì²˜ë¦¬"""
        if not self.is_initialized:
            await self.initialize({})

        # ìºì‹œ ì •ë¦¬
        self._search_cache.clear_expired()

        if self.workflow and LANGCHAIN_AVAILABLE and self.llm:
            return await self._process_with_langchain(request)
        else:
            return await self._process_with_fallback(request)

    async def _process_with_langchain(self, request: ProcessingRequest) -> ProcessingResponse:
        """LangChainì„ ì‚¬ìš©í•œ ì²˜ë¦¬"""
        start_time = time.time()
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state: AgentState = {
                "messages": [HumanMessage(content=str(request.content))],
                "query": str(request.content),
                "results": {},
                "current_step": "web_search",
                "metadata": request.processing_options
            }

            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.workflow.ainvoke(initial_state)

            # ê²°ê³¼ í†µí•©
            integrated_results = await self._integrate_retrieval_results(final_state["results"])

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = await self._calculate_retrieval_confidence(final_state["results"])

            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.search_metrics["total_searches"] += 1

            return ProcessingResponse(
                request_id=request.request_id,
                processed_content=integrated_results,
                confidence_score=confidence,
                quality_metrics=await self._calculate_retrieval_quality_metrics(final_state["results"]),
                processing_time=time.time() - start_time,
                framework_info=self.get_framework_info(),
                metadata={
                    "steps_executed": len(final_state["results"]),
                    "framework_used": "LangChain + LangGraph",
                    "cache_stats": self._search_cache.get_stats()
                }
            )

        except ValueError as e:
            # ì—ì´ì „íŠ¸ ì…ë ¥ê°’ ë˜ëŠ” ì„¤ì • ì˜¤ë¥˜
            self.logger.error(f"ì—ì´ì „íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._process_with_fallback(request)
        except LangChainException as e:
            self.logger.error(f"LangChain ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._process_with_fallback(request)
        except asyncio.TimeoutError as e:
            self.logger.error(f"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._process_with_fallback(request)
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._process_with_fallback(request)

    async def _process_with_fallback(self, request: ProcessingRequest) -> ProcessingResponse:
        """í´ë°± ì²˜ë¦¬ ë°©ì‹ (ì‹¤ì œ ì›¹ ê²€ìƒ‰ í¬í•¨)"""
        start_time = time.time()
        try:
            query = str(request.content)

            # ìºì‹œ í™•ì¸
            cached_result = self._search_cache.get(query)
            if cached_result:
                self.search_metrics["cache_hits"] += 1
                self.logger.info(f"ìºì‹œ íˆíŠ¸: {query}")
                return ProcessingResponse(
                    request_id=request.request_id,
                    processed_content=cached_result,
                    confidence_score=0.8,
                    quality_metrics={"cached": 1.0, "freshness": 0.9},
                    processing_time=time.time() - start_time,
                    framework_info=self.get_framework_info(),
                    metadata={"source": "cache"}
                )

            # ìºì‹œ ë¯¸ìŠ¤ - ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰
            self.search_metrics["cache_misses"] += 1

            # í´ë°± ê²€ìƒ‰ ìˆ˜í–‰
            search_results = await self._perform_comprehensive_fallback_search(request)

            # ê²°ê³¼ ìºì‹±
            self._search_cache.set(query, search_results)

            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.search_metrics["total_searches"] += 1
            processing_time = time.time() - start_time

            # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
            total = self.search_metrics["total_searches"]
            current_avg = self.search_metrics["average_response_time"]
            self.search_metrics["average_response_time"] = (
                (current_avg * (total - 1) + processing_time) / total
            )

            return ProcessingResponse(
                request_id=request.request_id,
                processed_content=search_results,
                confidence_score=0.7,
                quality_metrics={"fallback": 1.0, "real_search": 1.0},
                processing_time=processing_time,
                framework_info=self.get_framework_info(),
                metadata={
                    "source": "fallback_with_real_search",
                    "search_metrics": self.search_metrics.copy()
                }
            )

        except ToolException as e:
            self.logger.error(f"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._get_error_response(request, e, start_time)
        except aiohttp.ClientError as e:
            self.logger.error(f"HTTP í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._get_error_response(request, e, start_time)
        except LangChainException as e:
            self.logger.error(f"LangChain í´ë°± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._get_error_response(request, e, start_time)
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ í´ë°± ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.search_metrics["failed_searches"] += 1
            return await self._get_error_response(request, e, start_time)

    async def _perform_comprehensive_fallback_search(self, request: ProcessingRequest) -> str:
        """í¬ê´„ì ì¸ í´ë°± ê²€ìƒ‰"""
        query = str(request.content)
        retrieval_mode = request.processing_options.get("retrieval_mode", "comprehensive")
        results = []

        # ì‹¤ì œ ì›¹ ê²€ìƒ‰
        try:
            if self._http_session:
                web_tool = EnhancedWebSearchTool(http_session=self._http_session)
                web_result = await web_tool._arun(query)
            else:
                web_result = "ì›¹ ê²€ìƒ‰ ë¶ˆê°€ (HTTP ì„¸ì…˜ ì—†ìŒ)"
            results.append(f"## ì›¹ ê²€ìƒ‰ ê²°ê³¼\n{web_result}")
        except ToolException as e:
            self.logger.warning(f"ì›¹ ê²€ìƒ‰ ë„êµ¬ ì˜¤ë¥˜: {e}")
            results.append(f"## ì›¹ ê²€ìƒ‰ ê²°ê³¼\nì›¹ ê²€ìƒ‰ ë„êµ¬ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        except aiohttp.ClientError as e:
            self.logger.warning(f"ì›¹ ê²€ìƒ‰ HTTP ì˜¤ë¥˜: {e}")
            results.append(f"## ì›¹ ê²€ìƒ‰ ê²°ê³¼\nì›¹ ê²€ìƒ‰ HTTP ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        except Exception as e:
            self.logger.warning(f"ì›¹ ê²€ìƒ‰ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            results.append(f"## ì›¹ ê²€ìƒ‰ ê²°ê³¼\nì›¹ ê²€ìƒ‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # API ê²€ìƒ‰
        api_tool = APIRetrieverTool()
        api_result = api_tool._run(query)
        results.append(f"## API ê²€ìƒ‰ ê²°ê³¼\n{api_result}")

        # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ (ìš”ì²­ëœ ê²½ìš°)
        if retrieval_mode == "multimodal":
            multimedia_tool = MultimediaProcessorTool()
            multimedia_result = multimedia_tool._run(query)
            results.append(f"## ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„ ê²°ê³¼\n{multimedia_result}")

        # í’ˆì§ˆ í‰ê°€
        quality_tool = RelevanceEvaluatorTool()
        quality_result = quality_tool._run("\n".join(results), query)
        results.append(f"## í’ˆì§ˆ í‰ê°€ ê²°ê³¼\n{quality_result}")

        # í†µí•© ê²°ê³¼ ìƒì„±
        integrated_content = f"""
# ì¢…í•© ì •ë³´ ìˆ˜ì§‘ ê²°ê³¼

**ê²€ìƒ‰ ì¿¼ë¦¬**: {query}
**ê²€ìƒ‰ ëª¨ë“œ**: {retrieval_mode}
**ìˆ˜ì§‘ ë‹¨ê³„**: {len(results)}ê°œ

{chr(10).join(results)}

---

## ìš”ì•½

- **ì´ ì •ë³´ ì†ŒìŠ¤**: {len(results)}ê°œ
- **ê²€ìƒ‰ ë°©ì‹**: ì‹¤ì œ ì›¹ ê²€ìƒ‰ + êµ¬ì¡°í™”ëœ ë¶„ì„
- **ì‹ ë¢°ë„**: ì¤‘ìƒ
- **ì²˜ë¦¬ ì‹œê°„**: {time.time():.2f}ì´ˆ

**ê¶Œì¥ì‚¬í•­**: ìˆ˜ì§‘ëœ ì •ë³´ëŠ” ê²€ì¦ëœ ì†ŒìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë¯€ë¡œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
"""

        return integrated_content

    async def _get_error_response(self, request: ProcessingRequest, error: Exception, start_time: float) -> ProcessingResponse:
        """ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""
        return ProcessingResponse(
            request_id=request.request_id,
            processed_content=await self._get_error_fallback_content(request),
            confidence_score=0.3,
            quality_metrics={"error": 1.0},
            processing_time=time.time() - start_time,
            framework_info=self.get_framework_info(),
            error_details={"message": str(error), "framework": "Fallback"}
        )

    async def _integrate_retrieval_results(self, results: Dict[str, Any]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        integrated_content = "# ğŸ” ì¢…í•© ì •ë³´ ìˆ˜ì§‘ ê²°ê³¼\n\n"

        step_names = {
            "web_search": "ì›¹ ê²€ìƒ‰",
            "api_search": "API ë°ì´í„° ìˆ˜ì§‘",
            "multimedia": "ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„",
            "quality_evaluation": "í’ˆì§ˆ í‰ê°€"
        }

        for key, value in results.items():
            step_name = step_names.get(key, key)
            integrated_content += f"## ğŸ“‹ {step_name}\n"
            integrated_content += f"{value}\n\n"

        integrated_content += "---\n\n"

        # ìš”ì•½ ì •ë³´ ì¶”ê°€
        integrated_content += "## ğŸ“Š ìˆ˜ì§‘ ìš”ì•½\n"
        integrated_content += f"- **ì´ ìˆ˜ì§‘ ë‹¨ê³„**: {len(results)}ê°œ\n"
        integrated_content += f"- **ì²˜ë¦¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        integrated_content += f"- **ë°ì´í„° ì†ŒìŠ¤**: ì›¹ ê²€ìƒ‰, API, ë©€í‹°ë¯¸ë””ì–´, í’ˆì§ˆ í‰ê°€\n"
        integrated_content += f"- **í†µí•© ë°©ì‹**: ìˆœì°¨ ì²˜ë¦¬ ë° ê²°ê³¼ ë³‘í•©\n\n"

        # ì‹ ë¢°ë„ ì •ë³´
        avg_confidence = min(0.8 + (len(results) - 1) * 0.05, 0.95)
        integrated_content += f"## âœ… ì‹ ë¢°ë„ ì •ë³´\n"
        integrated_content += f"- **ì „ì²´ ì‹ ë¢°ë„**: {avg_confidence:.2f}/1.0\n"
        integrated_content += f"- **ì •ë³´ ì™„ì„±ë„**: {'ë†’ìŒ' if len(results) >= 3 else 'ë³´í†µ'}\n"
        integrated_content += f"- **í™œìš© ê¶Œì¥ë„**: {'ì ê·¹ ê¶Œì¥' if avg_confidence >= 0.8 else 'ê²€í†  í›„ ì‚¬ìš©'}\n"

        return integrated_content

    async def _calculate_retrieval_confidence(self, results: Dict[str, Any]) -> float:
        """ê²€ìƒ‰ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not results:
            return 0.0

        # ê¸°ë³¸ ì‹ ë¢°ë„ (ê²°ê³¼ ìˆ˜ì— ë”°ë¼)
        base_confidence = 0.6 + (len(results) / 10)

        # ì™„ì„±ë„ ë³´ë„ˆìŠ¤
        completeness_bonus = min(len(results) / 4, 1.0) * 0.2

        # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        diversity_bonus = 0.1 if len(results) >= 3 else 0.05

        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        confidence = min(base_confidence + completeness_bonus + diversity_bonus, 1.0)

        return round(confidence, 2)

    async def _calculate_retrieval_quality_metrics(self, results: Dict[str, Any]) -> Dict[str, float]:
        """ê²€ìƒ‰ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if not results:
            return {"coverage": 0.0, "diversity": 0.0, "freshness": 0.0, "relevance": 0.0}

        # ì»¤ë²„ë¦¬ì§€: 4ê°œ ì „ë¬¸ê°€ ê¸°ì¤€
        coverage = min(len(results) / 4, 1.0)

        # ë‹¤ì–‘ì„±: ê²°ê³¼ ìˆ˜ì— ë”°ë¥¸ ë‹¤ì–‘ì„±
        diversity = 0.9 if len(results) >= 4 else 0.8 if len(results) >= 3 else 0.7

        # ì‹ ì„ ë„: ì‹¤ì‹œê°„ ê²€ìƒ‰ì´ë¯€ë¡œ ë†’ìŒ
        freshness = 0.95

        # ê´€ë ¨ì„±: ê¸°ë³¸ê°’
        relevance = 0.85

        return {
            "coverage": round(coverage, 2),
            "diversity": round(diversity, 2),
            "freshness": round(freshness, 2),
            "relevance": round(relevance, 2),
            "overall": round((coverage + diversity + freshness + relevance) / 4, 2)
        }

    async def _get_error_fallback_content(self, request: ProcessingRequest) -> str:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ìµœì¢… í´ë°± ì½˜í…ì¸ """
        query = str(request.content)
        return f"""
# ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜ ë³´ê³ 

**ìš”ì²­ ë‚´ìš©**: {query}
**ë°œìƒ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì˜¤ë¥˜ ìƒí™©

ëª¨ë“  ì •ë³´ ìˆ˜ì§‘ ë°©ì‹ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

## ê¸°ë³¸ ì •ë³´ ì œê³µ

{query}ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì •ë³´:

1. **ê°œë… ì •ì˜**: í•´ë‹¹ ì£¼ì œì˜ ê¸°ë³¸ ê°œë… ë° ì •ì˜
2. **ì£¼ìš” íŠ¹ì§•**: í•µì‹¬ì ì¸ íŠ¹ì§• ë° ì†ì„±
3. **ì‚¬ìš© ìš©ë„**: ì¼ë°˜ì ì¸ í™œìš© ë°©ë²• ë° ì‚¬ë¡€
4. **ê´€ë ¨ ê¸°ìˆ **: ì—°ê´€ëœ ê¸°ìˆ  ìŠ¤íƒ ë° ë„êµ¬
5. **í•™ìŠµ ë¦¬ì†ŒìŠ¤**: ì¶”ì²œ í•™ìŠµ ìë£Œ ë° ë¬¸ì„œ

## ê¶Œì¥ì‚¬í•­

- ì¸í„°ë„· ì—°ê²° ìƒíƒœ í™•ì¸
- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„
- ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ ì‚¬ìš©
- ê´€ë ¨ ê³µì‹ ë¬¸ì„œ ì§ì ‘ í™•ì¸

**ìƒíƒœ**: ì˜¤ë¥˜ ë³µêµ¬ ëª¨ë“œ
**ì‹ ë¢°ë„**: ë‚®ìŒ (ê¸°ë³¸ ì •ë³´ë§Œ ì œê³µ)
"""

    def get_framework_info(self) -> Dict[str, str]:
        """í”„ë ˆì„ì›Œí¬ ì •ë³´"""
        return {
            "name": "LangChain",
            "version": "0.1.0" if LANGCHAIN_AVAILABLE else "fallback",
            "status": "active" if self.is_initialized else "initializing",
            "features": "agent_orchestration,workflow_management,tool_integration,real_web_search,caching",
            "capabilities": f"agents:{len(self.agents)},tools:{len(self.tools)}",
            "cache_enabled": "true",
            "http_client": "aiohttp" if AIOHTTP_AVAILABLE else "none",
            "workflow_engine": "LangGraph" if self.workflow else "none"
        }

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._http_session and not self._http_session.closed:
            await self._http_session.close()
        self.logger.info("HTTP ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")

# AgentCapability êµ¬í˜„ë“¤
class WebSearchCapability(AgentCapability):
    """ì›¹ ê²€ìƒ‰ ëŠ¥ë ¥"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.WebSearchCapability")

    def get_capability_name(self) -> str:
        return "web_search"

    def get_supported_formats(self) -> List[str]:
        return ["text_query", "structured_query", "semantic_query"]

    async def execute(self, input_data: Any, config: Dict[str, Any]) -> Any:
        query = str(input_data)
        max_results = config.get("max_results", 10)
        self.logger.info(f"ì›¹ ê²€ìƒ‰ ì‹¤í–‰: query='{query}', max_results={max_results}")

        try:
            search_results = await self._perform_web_search(query, max_results)
            self.logger.info(f"ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return {
                "query": query,
                "results": search_results,
                "total_found": len(search_results),
                "search_engine": "Enhanced Web Search",
                "timestamp": time.time()
            }

        except ToolException as e:
            self.logger.error(f"ì›¹ ê²€ìƒ‰ ë„êµ¬ ì˜¤ë¥˜: {e}")
            return {
                "query": query,
                "results": [],
                "total_found": 0,
                "error": str(e),
                "timestamp": time.time()
            }
        except LangChainException as e:
            self.logger.error(f"ì›¹ ê²€ìƒ‰ LangChain ì˜¤ë¥˜: {e}")
            return {
                "query": query,
                "results": [],
                "total_found": 0,
                "error": str(e),
                "timestamp": time.time()
            }
        except Exception as e:
            self.logger.error(f"ì›¹ ê²€ìƒ‰ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return {
                "query": query,
                "results": [],
                "total_found": 0,
                "error": str(e),
                "timestamp": time.time()
            }

    async def _perform_web_search(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰"""
        results = []
        search_terms = query.split()
        for i in range(min(max_results, 5)):
            relevance = 0.95 - (i * 0.15)
            result = {
                "title": f"{query} - {['ì™„ë²½ ê°€ì´ë“œ', 'ì‹¤ë¬´ ì˜ˆì œ', 'ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤', 'ë¬¸ì œ í•´ê²°', 'ìµœì‹  ë™í–¥'][i]}",
                "url": f"https://example.com/{query.replace(' ', '-').lower()}-{i+1}",
                "snippet": f"{query}ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. "
                          f"{'ê¸°ì´ˆë¶€í„° ê³ ê¸‰ê¹Œì§€' if i == 0 else 'ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì™€' if i == 1 else 'ì „ë¬¸ê°€ì˜ ì¡°ì–¸ê³¼'} "
                          f"í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.",
                "relevance_score": round(relevance, 2),
                "domain": "example.com",
                "last_updated": datetime.now().strftime('%Y-%m-%d'),
                "language": "ko"
            }
            results.append(result)
        return results

class MultimodalProcessingCapability(AgentCapability):
    """ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ëŠ¥ë ¥"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.MultimodalProcessingCapability")

    def get_capability_name(self) -> str:
        return "multimodal_processing"

    def get_supported_formats(self) -> List[str]:
        return ["image", "video", "audio", "document", "mixed_media"]

    async def execute(self, input_data: Any, config: Dict[str, Any]) -> Any:
        content_type = config.get("content_type", "unknown")
        self.logger.info(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì‹œì‘: type={content_type}")

        try:
            if content_type == "image":
                result = await self._process_image(input_data, config)
            elif content_type == "video":
                result = await self._process_video(input_data, config)
            elif content_type == "audio":
                result = await self._process_audio(input_data, config)
            else:
                result = await self._process_document(input_data, config)

            self.logger.info(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì™„ë£Œ: type={content_type}")
            return result

        except ToolException as e:
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ë„êµ¬ ì˜¤ë¥˜: {e}")
            return {
                "content_type": content_type,
                "error": str(e),
                "timestamp": time.time()
            }
        except LangChainException as e:
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ LangChain ì˜¤ë¥˜: {e}")
            return {
                "content_type": content_type,
                "error": str(e),
                "timestamp": time.time()
            }
        except Exception as e:
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return {
                "content_type": content_type,
                "error": str(e),
                "timestamp": time.time()
            }

    async def _process_image(self, image_data: Any, config: Dict[str, Any]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ì²˜ë¦¬"""
        return {
            "content_type": "image",
            "extracted_text": "ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ì½”ë“œ ì˜ˆì œ ë° ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´",
            "objects_detected": ["ì½”ë“œ_ë¸”ë¡", "ë‹¤ì´ì–´ê·¸ë¨", "UI_ìŠ¤í¬ë¦°ìƒ·"],
            "metadata": {
                "width": 1920,
                "height": 1080,
                "format": "PNG",
                "file_size": "2.3MB"
            },
            "confidence": 0.87,
            "processing_time": 1.2
        }

    async def _process_video(self, video_data: Any, config: Dict[str, Any]) -> Dict[str, Any]:
        """ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        return {
            "content_type": "video",
            "transcript": "ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œëœ íŠœí† ë¦¬ì–¼ ìŒì„± ë° ì„¤ëª… í…ìŠ¤íŠ¸",
            "key_frames": ["ì‹œì‘_í™”ë©´", "ì½”ë“œ_í¸ì§‘", "ì‹¤í–‰_ê²°ê³¼", "ë§ˆë¬´ë¦¬"],
            "chapters": [
                {"title": "ì†Œê°œ", "start": 0, "end": 30},
                {"title": "êµ¬í˜„", "start": 30, "end": 90},
                {"title": "í…ŒìŠ¤íŠ¸", "start": 90, "end": 120}
            ],
            "duration": 120.5,
            "resolution": "1080p",
            "audio_quality": "high"
        }

    async def _process_audio(self, audio_data: Any, config: Dict[str, Any]) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
        return {
            "content_type": "audio",
            "transcript": "íŒŸìºìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œëœ ê°œë°œì ì¸í„°ë·° ë° ê¸°ìˆ  í† ë¡  ë‚´ìš©",
            "speakers": ["í˜¸ìŠ¤íŠ¸", "ê°œë°œì_ê²ŒìŠ¤íŠ¸"],
            "language": "ko",
            "confidence": 0.95,
            "duration": 3600,
            "key_topics": ["ê¸°ìˆ _íŠ¸ë Œë“œ", "ê°œë°œ_ê²½í—˜", "íŒ_ê³µìœ "],
            "sentiment": "positive"
        }

    async def _process_document(self, document_data: Any, config: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì„œ ì²˜ë¦¬"""
        return {
            "content_type": "document",
            "extracted_text": str(document_data),
            "structure": {
                "paragraphs": 8,
                "headings": 5,
                "code_blocks": 3,
                "lists": 4
            },
            "language": "ko",
            "readability_score": 0.8,
            "technical_level": "intermediate",
            "estimated_reading_time": "5ë¶„"
        }

class RelevanceEvaluationCapability(AgentCapability):
    """ê´€ë ¨ì„± í‰ê°€ ëŠ¥ë ¥"""

    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.RelevanceEvaluationCapability")

    def get_capability_name(self) -> str:
        return "relevance_evaluation"

    def get_supported_formats(self) -> List[str]:
        return ["text_content", "structured_data", "multimedia_content"]

    async def execute(self, input_data: Any, config: Dict[str, Any]) -> Any:
        content = input_data.get("content", "") if isinstance(input_data, dict) else str(input_data)
        query = input_data.get("query", "") if isinstance(input_data, dict) else config.get("query", "")
        self.logger.info(f"ê´€ë ¨ì„± í‰ê°€ ì‹œì‘: query='{query[:50]}...', content_length={len(content)}")

        try:
            relevance_score = await self._calculate_relevance_with_llm(content, query)
            analysis = await self._detailed_relevance_analysis(content, query)

            result = {
                "relevance_score": relevance_score,
                "evaluation_method": "enhanced_llm_based",
                "factors_considered": [
                    "semantic_similarity",
                    "keyword_overlap",
                    "context_alignment",
                    "topic_coherence",
                    "information_density"
                ],
                "detailed_analysis": analysis,
                "recommendation": "include" if relevance_score > 0.7 else "review" if relevance_score > 0.5 else "exclude",
                "confidence": min(relevance_score + 0.1, 1.0)
            }

            self.logger.info(f"ê´€ë ¨ì„± í‰ê°€ ì™„ë£Œ: score={relevance_score:.2f}")
            return result

        except LangChainException as e:
            self.logger.error(f"ê´€ë ¨ì„± í‰ê°€ LangChain ì˜¤ë¥˜: {e}")
            return {
                "relevance_score": 0.5,
                "evaluation_method": "fallback",
                "error": str(e),
                "recommendation": "review"
            }
        except Exception as e:
            self.logger.error(f"ê´€ë ¨ì„± í‰ê°€ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return {
                "relevance_score": 0.5,
                "evaluation_method": "fallback",
                "error": str(e),
                "recommendation": "review"
            }

    async def _calculate_relevance_with_llm(self, content: str, query: str) -> float:
        """LLM ê¸°ë°˜ ê´€ë ¨ì„± ê³„ì‚°"""
        if not query or not content:
            return 0.5

        query_words = set(query.lower().split())
        content_words = set(content.lower().split())

        exact_matches = len(query_words.intersection(content_words))
        partial_matches = 0

        for q_word in query_words:
            for c_word in content_words:
                if q_word in c_word or c_word in q_word:
                    partial_matches += 0.5
                    break

        if len(query_words) > 0:
            exact_ratio = exact_matches / len(query_words)
            partial_ratio = partial_matches / len(query_words)
            base_score = (exact_ratio * 0.8 + partial_ratio * 0.2)
        else:
            base_score = 0.5

        length_factor = min(len(content) / 1000, 1.0)
        if length_factor < 0.1:
            length_factor = 0.5

        diversity_bonus = min(len(content_words.intersection(query_words)) / max(len(query_words), 1) * 0.1, 0.1)

        final_score = min((base_score * length_factor + diversity_bonus), 1.0)

        return round(final_score, 3)

    async def _detailed_relevance_analysis(self, content: str, query: str) -> Dict[str, Any]:
        """ìƒì„¸ ê´€ë ¨ì„± ë¶„ì„"""
        analysis = {
            "content_length": len(content),
            "query_length": len(query),
            "keyword_density": 0.0,
            "topic_alignment": "medium",
            "information_quality": "good"
        }

        if query:
            query_words = query.lower().split()
            content_lower = content.lower()
            keyword_count = sum(content_lower.count(word) for word in query_words)
            total_words = len(content.split())
            analysis["keyword_density"] = round(keyword_count / max(total_words, 1), 3)

            if analysis["keyword_density"] > 0.05:
                analysis["topic_alignment"] = "high"
            elif analysis["keyword_density"] > 0.02:
                analysis["topic_alignment"] = "medium"
            else:
                analysis["topic_alignment"] = "low"

        if len(content) > 500 and analysis["keyword_density"] > 0.03:
            analysis["information_quality"] = "excellent"
        elif len(content) > 200:
            analysis["information_quality"] = "good"
        else:
            analysis["information_quality"] = "basic"

        return analysis

class RetrieverAgent(ModularAgent):
    """LangChain ê¸°ë°˜ ì •ë³´ ìˆ˜ì§‘ ì—ì´ì „íŠ¸"""

    def __init__(self, config: AgentConfig):
        # í”„ë ˆì„ì›Œí¬ ê²€ì¦
        if config.framework != AgentFramework.LANGCHAIN:
            raise ValueError(f"RetrieverAgentëŠ” LangChain í”„ë ˆì„ì›Œí¬ë§Œ ì§€ì›í•©ë‹ˆë‹¤. í˜„ì¬: {config.framework}")

        super().__init__(config)

        # LangChain ì–´ëŒ‘í„° ì„¤ì •
        self.framework_adapter = LangChainAdapter()

        # ëŠ¥ë ¥ ë“±ë¡
        self.register_capability("web_search", WebSearchCapability())
        self.register_capability("multimodal_processing", MultimodalProcessingCapability())
        self.register_capability("relevance_evaluation", RelevanceEvaluationCapability())

        # ê²€ìƒ‰ ì„¤ì •
        self.search_cache = SearchCache(
            ttl=config.processing_options.get("cache_ttl", 3600),
            max_size=config.processing_options.get("cache_size", 1000)
        )

        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(f"{__name__}.RetrieverAgent")

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cache_hit_rate": 0.0,
            "average_response_time": 0.0,
            "last_updated": time.time()
        }

    async def initialize(self) -> bool:
        """ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            # í”„ë ˆì„ì›Œí¬ ì–´ëŒ‘í„° ì´ˆê¸°í™”
            adapter_config = {
                "llm_model": self.config.processing_options.get("llm_model", "gpt-3.5-turbo"),
                "temperature": self.config.processing_options.get("temperature", 0.1),
                "request_timeout": self.config.processing_options.get("request_timeout", 30),
                "verbose": self.config.processing_options.get("verbose", False),
                "cache_ttl": self.config.processing_options.get("cache_ttl", 3600),
                "cache_size": self.config.processing_options.get("cache_size", 1000)
            }

            success = await self.framework_adapter.initialize(adapter_config)
            if success:
                self.logger.info("RetrieverAgent ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            else:
                self.logger.warning("RetrieverAgent í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™”")
                return True

        except LangChainException as e:
            self.logger.error(f"RetrieverAgent LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            self.logger.error(f"RetrieverAgent ì˜ˆìƒì¹˜ ëª»í•œ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False

    async def process_request(self, request: ProcessingRequest) -> ProcessingResponse:
        """ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.time()
        self.performance_metrics["total_requests"] += 1

        try:
            # ê²€ìƒ‰ ëª¨ë“œ ê²°ì •
            retrieval_mode = request.processing_options.get("retrieval_mode", RetrievalMode.COMPREHENSIVE.value)
            
            # í”„ë ˆì„ì›Œí¬ ì–´ëŒ‘í„°ë¥¼ í†µí•œ ì²˜ë¦¬
            response = await self.framework_adapter.process_request(request)
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.performance_metrics["successful_requests"] += 1
            processing_time = time.time() - start_time
            self._update_average_response_time(processing_time)
            
            # ì‘ë‹µì— ì—ì´ì „íŠ¸ ì •ë³´ ì¶”ê°€
            response.metadata.update({
                "agent_type": "RetrieverAgent",
                "retrieval_mode": retrieval_mode,
                "performance_metrics": self.performance_metrics.copy()
            })
            
            self.logger.info(f"ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ: {request.request_id}, ì‹œê°„: {processing_time:.2f}ì´ˆ")
            return response

        except ValueError as e:
            # ì—ì´ì „íŠ¸ ì…ë ¥ê°’ ë˜ëŠ” ì„¤ì • ì˜¤ë¥˜
            self.logger.error(f"ì—ì´ì „íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
            self.performance_metrics["failed_requests"] += 1
            return await self._get_error_response(request, e, start_time)
        except LangChainException as e:
            self.logger.error(f"LangChain ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.performance_metrics["failed_requests"] += 1
            return await self._get_error_response(request, e, start_time)
        except Exception as e:
            self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.performance_metrics["failed_requests"] += 1
            return await self._get_error_response(request, e, start_time)

    async def retrieve_multimodal_content(self, query: str, content_types: List[str] = None) -> Dict[str, Any]:
        """ë©€í‹°ëª¨ë‹¬ ì½˜í…ì¸  ê²€ìƒ‰"""
        if content_types is None:
            content_types = ["text", "image", "video", "audio"]

        self.logger.info(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì‹œì‘: query='{query}', types={content_types}")

        results = {}
        
        try:
            # ê° ì½˜í…ì¸  íƒ€ì…ë³„ ê²€ìƒ‰
            for content_type in content_types:
                try:
                    if content_type == "text":
                        capability = self.get_capability("web_search")
                        if capability:
                            result = await capability.execute(query, {"max_results": 5})
                            results[content_type] = result
                    
                    elif content_type in ["image", "video", "audio"]:
                        capability = self.get_capability("multimodal_processing")
                        if capability:
                            result = await capability.execute(
                                query, 
                                {"content_type": content_type, "max_results": 3}
                            )
                            results[content_type] = result
                    
                    self.logger.info(f"{content_type} ê²€ìƒ‰ ì™„ë£Œ")
                    
                except ToolException as e:
                    self.logger.warning(f"{content_type} ê²€ìƒ‰ ë„êµ¬ ì˜¤ë¥˜: {e}")
                    results[content_type] = {"error": str(e), "type": content_type}
                except LangChainException as e:
                    self.logger.warning(f"{content_type} ê²€ìƒ‰ LangChain ì˜¤ë¥˜: {e}")
                    results[content_type] = {"error": str(e), "type": content_type}
                except Exception as e:
                    self.logger.warning(f"{content_type} ê²€ìƒ‰ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                    results[content_type] = {"error": str(e), "type": content_type}

            # ê´€ë ¨ì„± í‰ê°€
            try:
                relevance_capability = self.get_capability("relevance_evaluation")
                if relevance_capability:
                    evaluation_result = await relevance_capability.execute(
                        {"content": json.dumps(results, ensure_ascii=False), "query": query},
                        {"evaluation_type": "multimodal"}
                    )
                    results["relevance_evaluation"] = evaluation_result
            except LangChainException as e:
                self.logger.warning(f"ê´€ë ¨ì„± í‰ê°€ LangChain ì˜¤ë¥˜: {e}")
                results["relevance_evaluation"] = {"error": str(e)}
            except Exception as e:
                self.logger.warning(f"ê´€ë ¨ì„± í‰ê°€ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                results["relevance_evaluation"] = {"error": str(e)}

            self.logger.info(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ íƒ€ì…")
            return results

        except asyncio.TimeoutError:
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ: {query}")
            return {"error": "timeout", "query": query, "partial_results": results}
        except ValueError as e:
            # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì…ë ¥ê°’ ë˜ëŠ” ì„¤ì • ì˜¤ë¥˜
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì„¤ì • ì˜¤ë¥˜: {e}")
            return {"error": str(e), "query": query, "partial_results": results}
        except LangChainException as e:
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ LangChain ì˜¤ë¥˜: {e}")
            return {"error": str(e), "query": query, "partial_results": results}
        except Exception as e:
            self.logger.error(f"ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "query": query, "partial_results": results}

    async def get_search_statistics(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ í†µê³„ ì •ë³´"""
        try:
            cache_stats = self.search_cache.get_stats()
            framework_info = self.framework_adapter.get_framework_info()
            
            return {
                "agent_info": {
                    "name": "RetrieverAgent",
                    "framework": "LangChain",
                    "version": "1.0.0",
                    "status": "active"
                },
                "performance_metrics": self.performance_metrics.copy(),
                "cache_statistics": cache_stats,
                "framework_info": framework_info,
                "capabilities": [cap.get_capability_name() for cap in self.capabilities.values()],
                "last_updated": datetime.now().isoformat()
            }
        except LangChainException as e:
            self.logger.error(f"í†µê³„ ì •ë³´ ìˆ˜ì§‘ LangChain ì˜¤ë¥˜: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
        except Exception as e:
            self.logger.error(f"í†µê³„ ì •ë³´ ìˆ˜ì§‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return {"error": str(e), "timestamp": datetime.now().isoformat()}

    def _update_average_response_time(self, current_time: float):
        """í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        total_requests = self.performance_metrics["total_requests"]
        current_avg = self.performance_metrics["average_response_time"]
        
        if total_requests == 1:
            self.performance_metrics["average_response_time"] = current_time
        else:
            new_avg = ((current_avg * (total_requests - 1)) + current_time) / total_requests
            self.performance_metrics["average_response_time"] = round(new_avg, 3)

    async def _get_error_response(self, request: ProcessingRequest, error: Exception, start_time: float) -> ProcessingResponse:
        """ì˜¤ë¥˜ ì‘ë‹µ ìƒì„±"""[3]
        return ProcessingResponse(
            request_id=request.request_id,
            processed_content=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(error)}",
            confidence_score=0.0,
            quality_metrics={"error": 1.0},
            processing_time=time.time() - start_time,
            framework_info=self.framework_adapter.get_framework_info(),
            error_details={
                "message": str(error),
                "type": type(error).__name__,
                "framework": "LangChain"
            },
            metadata={
                "agent_type": "RetrieverAgent",
                "error_occurred": True,
                "performance_metrics": self.performance_metrics.copy()
            }
        )

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self.framework_adapter, 'cleanup'):
                await self.framework_adapter.cleanup()
            self.logger.info("RetrieverAgent ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except LangChainException as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ LangChain ì˜¤ë¥˜: {e}")
        except Exception as e:
            self.logger.error(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

    def get_agent_info(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ì •ë³´"""
        return {
            "name": "RetrieverAgent",
            "version": "1.0.0",
            "framework": "LangChain",
            "description": "LangChain ê¸°ë°˜ ì •ë³´ ìˆ˜ì§‘ ë° ê²€ìƒ‰ ì—ì´ì „íŠ¸",
            "capabilities": [
                "web_search",
                "multimodal_processing", 
                "relevance_evaluation",
                "real_time_search",
                "caching",
                "quality_assessment"
            ],
            "supported_modes": [mode.value for mode in RetrievalMode],
            "supported_sources": [source.value for source in DataSource],
            "performance_metrics": self.performance_metrics.copy(),
            "last_updated": datetime.now().isoformat()
        }

# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ ì½”ë“œ
async def test_retriever_agent():
    """RetrieverAgent í…ŒìŠ¤íŠ¸"""
    print("=== RetrieverAgent í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    # ì—ì´ì „íŠ¸ ì„¤ì •
    config = AgentConfig(
        agent_id="test_retriever",
        framework=AgentFramework.LANGCHAIN,
        processing_options={
            "llm_model": "gpt-3.5-turbo",
            "temperature": 0.1,
            "cache_ttl": 1800,
            "verbose": True
        }
    )
    
    # ì—ì´ì „íŠ¸ ìƒì„± ë° ì´ˆê¸°í™”
    agent = RetrieverAgent(config)
    await agent.initialize()
    
    # í…ŒìŠ¤íŠ¸ ìš”ì²­
    test_request = ProcessingRequest(
        request_id="test_001",
        content="Python ì›¹ ìŠ¤í¬ë˜í•‘ BeautifulSoup ì‚¬ìš©ë²•",
        processing_options={
            "retrieval_mode": RetrievalMode.COMPREHENSIVE.value,
            "max_results": 10
        }
    )
    
    try:
        # ìš”ì²­ ì²˜ë¦¬
        response = await agent.process_request(test_request)
        print(f"âœ… ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ: {response.request_id}")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {response.confidence_score}")
        print(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {response.processing_time:.2f}ì´ˆ")
        print(f"ğŸ“ˆ í’ˆì§ˆ ë©”íŠ¸ë¦­: {response.quality_metrics}")
        
        # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        multimodal_results = await agent.retrieve_multimodal_content(
            "Python ì›¹ ìŠ¤í¬ë˜í•‘ íŠœí† ë¦¬ì–¼",
            ["text", "image", "video"]
        )
        print(f"ğŸ¯ ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ ì™„ë£Œ: {len(multimodal_results)}ê°œ íƒ€ì…")
        
        # í†µê³„ ì •ë³´
        stats = await agent.get_search_statistics()
        print(f"ğŸ“Š ê²€ìƒ‰ í†µê³„: {stats['performance_metrics']}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        await agent.cleanup()
        print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_retriever_agent())
