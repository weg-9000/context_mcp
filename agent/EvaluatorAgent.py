# EvaluatorAgent - LangChain í”„ë ˆì„ì›Œí¬ ê¸°ë°˜ êµ¬í˜„
# í”„ë ˆì„ì›Œí¬: LangChain (í‰ê°€ ë„êµ¬ ìƒíƒœê³„ì™€ ì²´ì¸ êµ¬ì„±ì— íŠ¹í™”)

import asyncio
import json
import time
import re
from typing import Dict, List, Any, Optional, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

# LangChain ê´€ë ¨ imports
try:
    from langchain.llms import OpenAI
    from langchain.chat_models import ChatOpenAI
    from langchain.chains import LLMChain
    from langchain.prompts import PromptTemplate, ChatPromptTemplate
    from langchain.schema import BaseOutputParser
    from langchain.evaluation import load_evaluator, EvaluatorType
    from langchain.callbacks import StdOutCallbackHandler
    from langchain.memory import ConversationBufferMemory
    LANGCHAIN_AVAILABLE = True
except ImportError:
    # LangChainì´ ì—†ì„ ë•Œì˜ í´ë°± êµ¬í˜„
    LANGCHAIN_AVAILABLE = False
    print("LangChain not available, using fallback implementation")

from modular_agent_architecture import ProcessingMode
from modular_agent_architecture import (
    ModularAgent, AgentConfig, ProcessingRequest, ProcessingResponse,
    FrameworkAdapter, AgentCapability, AgentFramework
)


class EvaluationDimension(Enum):
    """í‰ê°€ ì°¨ì›"""
    FACTUAL_ACCURACY = "factual_accuracy"         # ì‚¬ì‹¤ ì •í™•ì„±
    LOGICAL_CONSISTENCY = "logical_consistency"   # ë…¼ë¦¬ì  ì¼ê´€ì„±
    RELEVANCE = "relevance"                       # ê´€ë ¨ì„±
    COMPLETENESS = "completeness"                 # ì™„ì „ì„±
    CLARITY = "clarity"                          # ëª…í™•ì„±
    OBJECTIVITY = "objectivity"                  # ê°ê´€ì„±
    SAFETY = "safety"                            # ì•ˆì „ì„±
    BIAS_DETECTION = "bias_detection"            # í¸í–¥ íƒì§€


class HallucinationType(Enum):
    """í™˜ê° ìœ í˜•"""
    FACTUAL_ERROR = "factual_error"               # ì‚¬ì‹¤ ì˜¤ë¥˜
    TEMPORAL_INCONSISTENCY = "temporal_inconsistency"  # ì‹œê°„ì  ë¶ˆì¼ì¹˜
    LOGICAL_CONTRADICTION = "logical_contradiction"    # ë…¼ë¦¬ì  ëª¨ìˆœ
    SOURCE_FABRICATION = "source_fabrication"     # ì¶œì²˜ ì¡°ì‘
    STATISTICAL_ERROR = "statistical_error"       # í†µê³„ ì˜¤ë¥˜
    CONTEXTUAL_MISALIGNMENT = "contextual_misalignment"  # ë¬¸ë§¥ ë¶€ì í•©


@dataclass
class EvaluationCriteria:
    """í‰ê°€ ê¸°ì¤€"""
    criteria_id: str
    name: str
    description: str
    weight: float
    threshold: float
    evaluation_prompt: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class EvaluationResult:
    """í‰ê°€ ê²°ê³¼"""
    evaluation_id: str
    content_evaluated: Any
    dimension_scores: Dict[EvaluationDimension, float]
    hallucination_detected: bool
    hallucination_types: List[HallucinationType]
    overall_confidence: float
    quality_grade: str
    improvement_suggestions: List[str]
    guardrail_violations: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


class LangChainAdapter(FrameworkAdapter):
    """LangChain í”„ë ˆì„ì›Œí¬ ì–´ëŒ‘í„°"""
    
    def __init__(self):
        self.llm: Optional[Any] = None
        self.chat_model: Optional[Any] = None
        self.evaluators: Dict[str, Any] = {}
        self.evaluation_chains: Dict[str, Any] = {}
        self.guardrail_chains: Dict[str, Any] = {}
        self.is_initialized = False
    
    def get_framework_name(self) -> str:
        return "LangChain"
    
    async def initialize(self, config: Dict[str, Any]) -> bool:
        """LangChain ì´ˆê¸°í™”"""
        try:
            if not LANGCHAIN_AVAILABLE:
                return await self._initialize_fallback(config)
            
            # LLM ëª¨ë¸ ì„¤ì •
            api_key = config.get("openai_api_key", "default-key")
            model_name = config.get("model_name", "gpt-3.5-turbo")
            
            self.llm = OpenAI(
                openai_api_key=api_key,
                temperature=config.get("temperature", 0.1),
                max_tokens=config.get("max_tokens", 1000)
            )
            
            self.chat_model = ChatOpenAI(
                openai_api_key=api_key,
                model_name=model_name,
                temperature=config.get("temperature", 0.1)
            )
            
            # í‰ê°€ê¸°ë“¤ ì´ˆê¸°í™”
            await self._initialize_evaluators(config)
            
            # í‰ê°€ ì²´ì¸ë“¤ êµ¬ì„±
            await self._build_evaluation_chains(config)
            
            # ê°€ë“œë ˆì¼ ì²´ì¸ë“¤ êµ¬ì„±
            await self._build_guardrail_chains(config)
            
            self.is_initialized = True
            return True
            
        except Exception as e:
            print(f"LangChain ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return await self._initialize_fallback(config)
    
    async def _initialize_fallback(self, config: Dict[str, Any]) -> bool:
        """í´ë°± ì´ˆê¸°í™”"""
        self.is_initialized = True
        return True
    
    async def _initialize_evaluators(self, config: Dict[str, Any]):
        """í‰ê°€ê¸°ë“¤ ì´ˆê¸°í™”"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        try:
            # ê¸°ë³¸ í‰ê°€ê¸°ë“¤
            self.evaluators["qa"] = load_evaluator(EvaluatorType.QA)
            self.evaluators["criteria"] = load_evaluator(EvaluatorType.CRITERIA)
            self.evaluators["labeled_criteria"] = load_evaluator(EvaluatorType.LABELED_CRITERIA)
            
            # ì‚¬ìš©ì ì •ì˜ í‰ê°€ê¸°ë“¤
            custom_evaluators = config.get("custom_evaluators", [])
            for evaluator_config in custom_evaluators:
                evaluator_name = evaluator_config["name"]
                self.evaluators[evaluator_name] = await self._create_custom_evaluator(evaluator_config)
                
        except Exception as e:
            print(f"í‰ê°€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def _create_custom_evaluator(self, config: Dict[str, Any]) -> Any:
        """ì‚¬ìš©ì ì •ì˜ í‰ê°€ê¸° ìƒì„±"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ í‰ê°€ê¸° ìƒì„± ë¡œì§
        return self.evaluators.get("criteria")  # í´ë°±
    
    async def _build_evaluation_chains(self, config: Dict[str, Any]):
        """í‰ê°€ ì²´ì¸ë“¤ êµ¬ì„±"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        # ì‚¬ì‹¤ ì •í™•ì„± í‰ê°€ ì²´ì¸
        factual_prompt = PromptTemplate(
            input_variables=["content", "reference"],
            template="""
            ë‹¤ìŒ ë‚´ìš©ì˜ ì‚¬ì‹¤ ì •í™•ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            
            í‰ê°€ ëŒ€ìƒ: {content}
            ì°¸ê³  ìë£Œ: {reference}
            
            í‰ê°€ ê¸°ì¤€:
            1. ì‚¬ì‹¤ ì •ë³´ì˜ ì •í™•ì„±
            2. ìˆ˜ì¹˜ ë°ì´í„°ì˜ ì •í™•ì„±
            3. ë‚ ì§œì™€ ì‹œê°„ ì •ë³´ì˜ ì •í™•ì„±
            4. ì¸ìš©ê³¼ ì¶œì²˜ì˜ ì‹ ë¢°ì„±
            
            ì ìˆ˜: [0.0-1.0]
            ì´ìœ : [í‰ê°€ ê·¼ê±°]
            """
        )
        
        self.evaluation_chains["factual_accuracy"] = LLMChain(
            llm=self.chat_model,
            prompt=factual_prompt,
            verbose=config.get("verbose", False)
        )
        
        # ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€ ì²´ì¸
        logical_prompt = PromptTemplate(
            input_variables=["content"],
            template="""
            ë‹¤ìŒ ë‚´ìš©ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            
            í‰ê°€ ëŒ€ìƒ: {content}
            
            í‰ê°€ ê¸°ì¤€:
            1. ë…¼ì¦ì˜ ë…¼ë¦¬ì  ì—°ê²°ì„±
            2. ì „ì œì™€ ê²°ë¡ ì˜ íƒ€ë‹¹ì„±
            3. ëª¨ìˆœì´ë‚˜ ìê¸° ë°˜ë°•ì˜ ë¶€ì¬
            4. ì¶”ë¡  ê³¼ì •ì˜ í•©ë¦¬ì„±
            
            ì ìˆ˜: [0.0-1.0]
            ëª¨ìˆœ ë°œê²¬: [ìˆìŒ/ì—†ìŒ]
            ì´ìœ : [í‰ê°€ ê·¼ê±°]
            """
        )
        
        self.evaluation_chains["logical_consistency"] = LLMChain(
            llm=self.chat_model,
            prompt=logical_prompt
        )
        
        # ê´€ë ¨ì„± í‰ê°€ ì²´ì¸
        relevance_prompt = PromptTemplate(
            input_variables=["content", "query"],
            template="""
            ë‹¤ìŒ ë‚´ìš©ì´ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ 0.0ì—ì„œ 1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.
            
            ì§ˆë¬¸: {query}
            ë‚´ìš©: {content}
            
            í‰ê°€ ê¸°ì¤€:
            1. ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì  ë‹µë³€ ì—¬ë¶€
            2. ê´€ë ¨ ì •ë³´ì˜ í¬í•¨ ì •ë„
            3. ë¶ˆí•„ìš”í•œ ì •ë³´ì˜ í¬í•¨ ì •ë„ (ê°ì  ìš”ì†Œ)
            4. ì§ˆë¬¸ ì˜ë„ì™€ì˜ ë¶€í•©ì„±
            
            ì ìˆ˜: [0.0-1.0]
            ì´ìœ : [í‰ê°€ ê·¼ê±°]
            """
        )
        
        self.evaluation_chains["relevance"] = LLMChain(
            llm=self.chat_model,
            prompt=relevance_prompt
        )
        
        # í™˜ê° íƒì§€ ì²´ì¸
        hallucination_prompt = PromptTemplate(
            input_variables=["content"],
            template="""
            ë‹¤ìŒ ë‚´ìš©ì—ì„œ í™˜ê°(hallucination)ì´ë‚˜ í—ˆìœ„ ì •ë³´ë¥¼ íƒì§€í•´ì£¼ì„¸ìš”.
            
            í‰ê°€ ëŒ€ìƒ: {content}
            
            íƒì§€ í•­ëª©:
            1. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì‚¬ì‹¤ì´ë‚˜ ë°ì´í„°
            2. í™•ì¸í•  ìˆ˜ ì—†ëŠ” ì¶œì²˜ë‚˜ ì¸ìš©
            3. ì‹œê°„ì  ëª¨ìˆœ (ì˜ˆ: ë¯¸ë˜ ì‚¬ê±´ì„ ê³¼ê±°í˜•ìœ¼ë¡œ ì„œìˆ )
            4. ë…¼ë¦¬ì  ëª¨ìˆœ
            5. ê³¼ì¥ë˜ê±°ë‚˜ ë¹„í˜„ì‹¤ì ì¸ ì£¼ì¥
            
            í™˜ê° íƒì§€: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            íƒì§€ëœ ìœ í˜•: [ìœ í˜• ëª©ë¡]
            êµ¬ì²´ì  ë‚´ìš©: [íƒì§€ëœ ë¶€ë¶„]
            ì‹ ë¢°ë„: [0.0-1.0]
            """
        )
        
        self.evaluation_chains["hallucination_detection"] = LLMChain(
            llm=self.chat_model,
            prompt=hallucination_prompt
        )
    
    async def _build_guardrail_chains(self, config: Dict[str, Any]):
        """ê°€ë“œë ˆì¼ ì²´ì¸ë“¤ êµ¬ì„±"""
        if not LANGCHAIN_AVAILABLE:
            return
        
        # ìœ í•´ ì½˜í…ì¸  íƒì§€ ì²´ì¸
        harmful_content_prompt = PromptTemplate(
            input_variables=["content"],
            template="""
            ë‹¤ìŒ ë‚´ìš©ì— ìœ í•´í•œ ìš”ì†Œê°€ ìˆëŠ”ì§€ ê²€ì‚¬í•´ì£¼ì„¸ìš”.
            
            ê²€ì‚¬ ëŒ€ìƒ: {content}
            
            ê²€ì‚¬ í•­ëª©:
            1. í­ë ¥ì  ë‚´ìš©
            2. ì°¨ë³„ì  í‘œí˜„
            3. í˜ì˜¤ ë°œì–¸
            4. ë¶ˆë²•ì  í™œë™ ì¡°ì¥
            5. ê°œì¸ì •ë³´ ë…¸ì¶œ
            
            ìœ í•´ì„± ë°œê²¬: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            ìœ í•´ ìš”ì†Œ: [ë°œê²¬ëœ ìš”ì†Œë“¤]
            ì‹¬ê°ë„: [ë‚®ìŒ/ë³´í†µ/ë†’ìŒ/ë§¤ìš°ë†’ìŒ]
            """
        )
        
        self.guardrail_chains["harmful_content"] = LLMChain(
            llm=self.chat_model,
            prompt=harmful_content_prompt
        )
        
        # í¸í–¥ íƒì§€ ì²´ì¸
        bias_detection_prompt = PromptTemplate(
            input_variables=["content"],
            template="""
            ë‹¤ìŒ ë‚´ìš©ì—ì„œ í¸í–¥(bias)ì„ íƒì§€í•´ì£¼ì„¸ìš”.
            
            ë¶„ì„ ëŒ€ìƒ: {content}
            
            í¸í–¥ ìœ í˜•:
            1. ì„±ë³„ í¸í–¥
            2. ì¸ì¢…/ë¯¼ì¡± í¸í–¥
            3. ì¢…êµì  í¸í–¥
            4. ì •ì¹˜ì  í¸í–¥
            5. ë¬¸í™”ì  í¸í–¥
            6. í™•ì¦ í¸í–¥
            
            í¸í–¥ ë°œê²¬: [ì˜ˆ/ì•„ë‹ˆì˜¤]
            í¸í–¥ ìœ í˜•: [ë°œê²¬ëœ í¸í–¥ë“¤]
            í¸í–¥ ì •ë„: [ì•½í•¨/ë³´í†µ/ê°•í•¨]
            ê°œì„  ì œì•ˆ: [í¸í–¥ ì œê±° ë°©ë²•]
            """
        )
        
        self.guardrail_chains["bias_detection"] = LLMChain(
            llm=self.chat_model,
            prompt=bias_detection_prompt
        )
    
    async def process_request(self, request: ProcessingRequest) -> ProcessingResponse:
        """LangChainì„ ì‚¬ìš©í•œ ìš”ì²­ ì²˜ë¦¬"""
        if not self.is_initialized:
            await self.initialize({})
        
        if self.evaluation_chains and LANGCHAIN_AVAILABLE:
            return await self._process_with_langchain(request)
        else:
            return await self._process_with_fallback(request)
    
    async def _process_with_langchain(self, request: ProcessingRequest) -> ProcessingResponse:
        """LangChainì„ ì‚¬ìš©í•œ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            content = request.content
            evaluation_config = request.processing_options
            
            # ì°¨ì›ë³„ í‰ê°€ ìˆ˜í–‰
            dimension_scores = await self._evaluate_all_dimensions(content, evaluation_config)
            
            # í™˜ê° íƒì§€
            hallucination_result = await self._detect_hallucinations(content)
            
            # ê°€ë“œë ˆì¼ ê²€ì‚¬
            guardrail_violations = await self._check_guardrails(content)
            
            # ì „ì²´ í‰ê°€ ê²°ê³¼ í†µí•©
            evaluation_result = await self._integrate_evaluation_results(
                content, dimension_scores, hallucination_result, guardrail_violations
            )
            
            return ProcessingResponse(
                request_id=request.request_id,
                processed_content=self._format_evaluation_report(evaluation_result),
                confidence_score=evaluation_result.overall_confidence,
                quality_metrics={dim.value: score for dim, score in evaluation_result.dimension_scores.items()},
                processing_time=time.time() - start_time,
                framework_info=self.get_framework_info(),
                metadata={
                    "hallucination_detected": evaluation_result.hallucination_detected,
                    "quality_grade": evaluation_result.quality_grade,
                    "guardrail_violations": evaluation_result.guardrail_violations,
                    "improvement_suggestions": evaluation_result.improvement_suggestions
                }
            )
            
        except Exception as e:
            return ProcessingResponse(
                request_id=request.request_id,
                processed_content=None,
                confidence_score=0.0,
                quality_metrics={"error": 1.0},
                processing_time=time.time() - start_time,
                framework_info=self.get_framework_info(),
                error_details={"message": str(e), "framework": "LangChain"}
            )
    
    async def _process_with_fallback(self, request: ProcessingRequest) -> ProcessingResponse:
        """í´ë°± ì²˜ë¦¬ ë°©ì‹"""
        start_time = time.time()
        
        # ê°„ë‹¨í•œ í‰ê°€ ì‹œë®¬ë ˆì´ì…˜
        content = str(request.content)
        
        evaluation_report = f"""
# ì½˜í…ì¸  í‰ê°€ ê²°ê³¼ (í´ë°± ëª¨ë“œ)

## í‰ê°€ ëŒ€ìƒ
{content[:200]}...

## í‰ê°€ ì ìˆ˜
- ì‚¬ì‹¤ ì •í™•ì„±: 0.8
- ë…¼ë¦¬ì  ì¼ê´€ì„±: 0.85
- ê´€ë ¨ì„±: 0.9
- ì™„ì „ì„±: 0.75

## ì „ì²´ í‰ê°€
- ì‹ ë¢°ë„: 0.82
- ë“±ê¸‰: B+
- í™˜ê° íƒì§€: ì—†ìŒ

â€» LangChain í”„ë ˆì„ì›Œí¬ê°€ ì—†ì„ ë•Œì˜ í´ë°± ê²°ê³¼ì…ë‹ˆë‹¤.
"""
        
        return ProcessingResponse(
            request_id=request.request_id,
            processed_content=evaluation_report,
            confidence_score=0.82,
            quality_metrics={"fallback": 1.0},
            processing_time=time.time() - start_time,
            framework_info=self.get_framework_info()
        )
    
    async def _evaluate_all_dimensions(self, content: Any, config: Dict[str, Any]) -> Dict[EvaluationDimension, float]:
        """ëª¨ë“  ì°¨ì›ì— ëŒ€í•œ í‰ê°€"""
        dimension_scores = {}
        
        # ì‚¬ì‹¤ ì •í™•ì„± í‰ê°€
        if "factual_accuracy" in self.evaluation_chains:
            try:
                result = await self.evaluation_chains["factual_accuracy"].arun(
                    content=str(content),
                    reference=config.get("reference_data", "")
                )
                score = self._extract_score_from_result(result)
                dimension_scores[EvaluationDimension.FACTUAL_ACCURACY] = score
            except:
                dimension_scores[EvaluationDimension.FACTUAL_ACCURACY] = 0.7
        
        # ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€
        if "logical_consistency" in self.evaluation_chains:
            try:
                result = await self.evaluation_chains["logical_consistency"].arun(content=str(content))
                score = self._extract_score_from_result(result)
                dimension_scores[EvaluationDimension.LOGICAL_CONSISTENCY] = score
            except:
                dimension_scores[EvaluationDimension.LOGICAL_CONSISTENCY] = 0.75
        
        # ê´€ë ¨ì„± í‰ê°€
        if "relevance" in self.evaluation_chains:
            try:
                result = await self.evaluation_chains["relevance"].arun(
                    content=str(content),
                    query=config.get("original_query", "")
                )
                score = self._extract_score_from_result(result)
                dimension_scores[EvaluationDimension.RELEVANCE] = score
            except:
                dimension_scores[EvaluationDimension.RELEVANCE] = 0.8
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ë‚˜ë¨¸ì§€ ì°¨ì›ë“¤ ì±„ìš°ê¸°
        default_scores = {
            EvaluationDimension.COMPLETENESS: 0.78,
            EvaluationDimension.CLARITY: 0.82,
            EvaluationDimension.OBJECTIVITY: 0.85,
            EvaluationDimension.SAFETY: 0.9
        }
        
        for dimension, default_score in default_scores.items():
            if dimension not in dimension_scores:
                dimension_scores[dimension] = default_score
        
        return dimension_scores
    
    async def _detect_hallucinations(self, content: Any) -> Dict[str, Any]:
        """í™˜ê° íƒì§€"""
        if "hallucination_detection" in self.evaluation_chains:
            try:
                result = await self.evaluation_chains["hallucination_detection"].arun(content=str(content))
                
                # ê²°ê³¼ íŒŒì‹±
                hallucination_detected = "ì˜ˆ" in result or "í™˜ê°" in result
                detected_types = self._extract_hallucination_types(result)
                confidence = self._extract_score_from_result(result)
                
                return {
                    "detected": hallucination_detected,
                    "types": detected_types,
                    "confidence": confidence,
                    "details": result
                }
            except:
                pass
        
        # í´ë°±: ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ íƒì§€
        content_str = str(content)
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ë“¤
        suspicious_patterns = [
            r'ì—°êµ¬ì— ë”°ë¥´ë©´.*í•˜ì§€ë§Œ êµ¬ì²´ì .*ì—†ë‹¤',
            r'ì „ë¬¸ê°€.*ë§í–ˆì§€ë§Œ.*í™•ì¸.*ì–´ë µë‹¤',
            r'\d{4}ë…„.*ì¼ì–´ë‚ .*ì˜ˆì •',  # ë¯¸ë˜ ì‚¬ê±´ì„ ê³¼ê±°í˜•ìœ¼ë¡œ
            r'100%.*í™•ì‹¤.*í•˜ì§€ë§Œ.*ê°€ëŠ¥ì„±.*ìˆë‹¤'  # ëª¨ìˆœì  í‘œí˜„
        ]
        
        detected_patterns = []
        for pattern in suspicious_patterns:
            if re.search(pattern, content_str):
                detected_patterns.append(pattern)
        
        return {
            "detected": len(detected_patterns) > 0,
            "types": [HallucinationType.FACTUAL_ERROR] if detected_patterns else [],
            "confidence": 0.6 if detected_patterns else 0.9,
            "details": f"íŒ¨í„´ ê¸°ë°˜ íƒì§€: {len(detected_patterns)}ê°œ ì˜ì‹¬ íŒ¨í„´"
        }
    
    async def _check_guardrails(self, content: Any) -> List[str]:
        """ê°€ë“œë ˆì¼ ê²€ì‚¬"""
        violations = []
        
        # ìœ í•´ ì½˜í…ì¸  ê²€ì‚¬
        if "harmful_content" in self.guardrail_chains:
            try:
                result = await self.guardrail_chains["harmful_content"].arun(content=str(content))
                if "ì˜ˆ" in result or "ìœ í•´" in result:
                    violations.append("harmful_content")
            except:
                pass
        
        # í¸í–¥ íƒì§€ ê²€ì‚¬
        if "bias_detection" in self.guardrail_chains:
            try:
                result = await self.guardrail_chains["bias_detection"].arun(content=str(content))
                if "ì˜ˆ" in result or "í¸í–¥" in result:
                    violations.append("bias_detected")
            except:
                pass
        
        # ì¶”ê°€ ê¸°ë³¸ ê²€ì‚¬ë“¤
        content_str = str(content).lower()
        
        # ê°œì¸ì •ë³´ íŒ¨í„´ ê²€ì‚¬
        privacy_patterns = [
            r'\d{3}-\d{4}-\d{4}',  # ì „í™”ë²ˆí˜¸
            r'\d{6}-[1-4]\d{6}',   # ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
            r'[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,}'  # ì´ë©”ì¼
        ]
        
        for pattern in privacy_patterns:
            if re.search(pattern, content_str):
                violations.append("privacy_violation")
                break
        
        return violations
    
    async def _integrate_evaluation_results(self, content: Any, dimension_scores: Dict, hallucination_result: Dict, guardrail_violations: List[str]) -> EvaluationResult:
        """í‰ê°€ ê²°ê³¼ í†µí•©"""
        # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
        overall_confidence = sum(dimension_scores.values()) / len(dimension_scores)
        
        # í™˜ê°ì´ íƒì§€ë˜ë©´ ì‹ ë¢°ë„ í¬ê²Œ ê°ì†Œ
        if hallucination_result["detected"]:
            overall_confidence *= 0.5
        
        # ê°€ë“œë ˆì¼ ìœ„ë°˜ì´ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ê°ì†Œ
        if guardrail_violations:
            overall_confidence *= (1.0 - len(guardrail_violations) * 0.1)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if overall_confidence >= 0.9:
            quality_grade = "A+"
        elif overall_confidence >= 0.8:
            quality_grade = "A"
        elif overall_confidence >= 0.7:
            quality_grade = "B+"
        elif overall_confidence >= 0.6:
            quality_grade = "B"
        elif overall_confidence >= 0.5:
            quality_grade = "C"
        else:
            quality_grade = "F"
        
        # ê°œì„  ì œì•ˆ ìƒì„±
        improvement_suggestions = self._generate_improvement_suggestions(dimension_scores, hallucination_result, guardrail_violations)
        
        return EvaluationResult(
            evaluation_id=f"eval_{int(time.time())}",
            content_evaluated=content,
            dimension_scores=dimension_scores,
            hallucination_detected=hallucination_result["detected"],
            hallucination_types=hallucination_result["types"],
            overall_confidence=overall_confidence,
            quality_grade=quality_grade,
            improvement_suggestions=improvement_suggestions,
            guardrail_violations=guardrail_violations,
            metadata={
                "evaluation_timestamp": datetime.now().isoformat(),
                "hallucination_confidence": hallucination_result["confidence"]
            }
        )
    
    def _extract_score_from_result(self, result: str) -> float:
        """ê²°ê³¼ì—ì„œ ì ìˆ˜ ì¶”ì¶œ"""
        # ì ìˆ˜ íŒ¨í„´ ì°¾ê¸°
        score_patterns = [
            r'ì ìˆ˜[:\s]*([0-9]\.[0-9])',
            r'([0-9]\.[0-9])',
            r'(\d+)/10',
            r'(\d+)%'
        ]
        
        for pattern in score_patterns:
            match = re.search(pattern, result)
            if match:
                score_str = match.group(1)
                try:
                    score = float(score_str)
                    # ì •ê·œí™” (0.0 ~ 1.0 ë²”ìœ„)
                    if score > 1.0:
                        if score <= 10:
                            score = score / 10.0
                        elif score <= 100:
                            score = score / 100.0
                    return min(max(score, 0.0), 1.0)
                except ValueError:
                    continue
        
        # íŒ¨í„´ì„ ì°¾ì§€ ëª»í•˜ë©´ ê¸°ë³¸ê°’
        return 0.75
    
    def _extract_hallucination_types(self, result: str) -> List[HallucinationType]:
        """ê²°ê³¼ì—ì„œ í™˜ê° ìœ í˜• ì¶”ì¶œ"""
        types = []
        
        type_keywords = {
            HallucinationType.FACTUAL_ERROR: ["ì‚¬ì‹¤ ì˜¤ë¥˜", "ì˜ëª»ëœ ì •ë³´", "ë¶€ì •í™•"],
            HallucinationType.TEMPORAL_INCONSISTENCY: ["ì‹œê°„", "ë‚ ì§œ", "ì‹œì "],
            HallucinationType.LOGICAL_CONTRADICTION: ["ëª¨ìˆœ", "ë…¼ë¦¬ì ", "ì¼ê´€ì„±"],
            HallucinationType.SOURCE_FABRICATION: ["ì¶œì²˜", "ì¸ìš©", "ì°¸ê³ "]
        }
        
        for hallucination_type, keywords in type_keywords.items():
            if any(keyword in result for keyword in keywords):
                types.append(hallucination_type)
        
        return types
    
    def _generate_improvement_suggestions(self, dimension_scores: Dict, hallucination_result: Dict, guardrail_violations: List[str]) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ì°¨ì›ë³„ ê°œì„  ì œì•ˆ
        for dimension, score in dimension_scores.items():
            if score < 0.7:
                if dimension == EvaluationDimension.FACTUAL_ACCURACY:
                    suggestions.append("ì‚¬ì‹¤ í™•ì¸ì„ ìœ„í•´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ë¥¼ ì¶”ê°€ë¡œ ì°¸ì¡°í•˜ì„¸ìš”.")
                elif dimension == EvaluationDimension.LOGICAL_CONSISTENCY:
                    suggestions.append("ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ ìœ„í•´ ì£¼ì¥ê³¼ ê·¼ê±° ê°„ì˜ ì—°ê²°ì„ ê°•í™”í•˜ì„¸ìš”.")
                elif dimension == EvaluationDimension.RELEVANCE:
                    suggestions.append("ì§ˆë¬¸ê³¼ ë” ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ì— ì§‘ì¤‘í•˜ì„¸ìš”.")
                elif dimension == EvaluationDimension.COMPLETENESS:
                    suggestions.append("ëˆ„ë½ëœ ì •ë³´ë‚˜ ê´€ì ì„ ì¶”ê°€ë¡œ í¬í•¨í•˜ì„¸ìš”.")
                elif dimension == EvaluationDimension.CLARITY:
                    suggestions.append("ë” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        
        # í™˜ê° ê´€ë ¨ ì œì•ˆ
        if hallucination_result["detected"]:
            suggestions.append("ê²€ì¦ë˜ì§€ ì•Šì€ ì •ë³´ë‚˜ ì£¼ì¥ì„ ì œê±°í•˜ê³  í™•ì‹¤í•œ ì‚¬ì‹¤ë§Œ í¬í•¨í•˜ì„¸ìš”.")
        
        # ê°€ë“œë ˆì¼ ìœ„ë°˜ ê´€ë ¨ ì œì•ˆ
        if "harmful_content" in guardrail_violations:
            suggestions.append("ìœ í•´í•œ í‘œí˜„ì´ë‚˜ ë‚´ìš©ì„ ì œê±°í•˜ì„¸ìš”.")
        if "bias_detected" in guardrail_violations:
            suggestions.append("í¸í–¥ì  í‘œí˜„ì„ ì¤‘ë¦½ì ì´ê³  ê· í˜• ì¡íŒ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•˜ì„¸ìš”.")
        if "privacy_violation" in guardrail_violations:
            suggestions.append("ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ë‚´ìš©ì„ ì œê±°í•˜ê±°ë‚˜ ìµëª…í™”í•˜ì„¸ìš”.")
        
        return suggestions
    
    def _format_evaluation_report(self, result: EvaluationResult) -> str:
        """í‰ê°€ ë³´ê³ ì„œ í˜•ì‹í™”"""
        report = f"""# ì½˜í…ì¸  í‰ê°€ ë³´ê³ ì„œ

## ì „ì²´ í‰ê°€
- **ì‹ ë¢°ë„**: {result.overall_confidence:.2f}
- **í’ˆì§ˆ ë“±ê¸‰**: {result.quality_grade}
- **í™˜ê° íƒì§€**: {'ì˜ˆ' if result.hallucination_detected else 'ì•„ë‹ˆì˜¤'}

## ì°¨ì›ë³„ ì ìˆ˜
"""
        
        for dimension, score in result.dimension_scores.items():
            report += f"- **{dimension.value}**: {score:.2f}\n"
        
        if result.hallucination_detected:
            report += f"\n## âš ï¸ í™˜ê° íƒì§€\n"
            report += f"- **íƒì§€ëœ ìœ í˜•**: {[t.value for t in result.hallucination_types]}\n"
        
        if result.guardrail_violations:
            report += f"\n## ğŸš« ê°€ë“œë ˆì¼ ìœ„ë°˜\n"
            for violation in result.guardrail_violations:
                report += f"- {violation}\n"
        
        if result.improvement_suggestions:
            report += f"\n## ğŸ’¡ ê°œì„  ì œì•ˆ\n"
            for i, suggestion in enumerate(result.improvement_suggestions, 1):
                report += f"{i}. {suggestion}\n"
        
        report += f"\n---\n*í‰ê°€ ì™„ë£Œ ì‹œê°„: {result.metadata.get('evaluation_timestamp', 'N/A')}*"
        
        return report
    
    def get_framework_info(self) -> Dict[str, str]:
        """í”„ë ˆì„ì›Œí¬ ì •ë³´"""
        return {
            "name": "LangChain",
            "version": "0.0.350" if LANGCHAIN_AVAILABLE else "fallback",
            "status": "active" if self.is_initialized else "initializing",
            "features": "evaluation_chains,guardrails,criteria_evaluation,hallucination_detection"
        }


class HallucinationDetectionCapability(AgentCapability):
    """í™˜ê° íƒì§€ ëŠ¥ë ¥"""
    
    def get_capability_name(self) -> str:
        return "hallucination_detection"
    
    def get_supported_formats(self) -> List[str]:
        return ["text", "structured_content", "claims", "statements"]
    
    async def execute(self, input_data: Any, config: Dict[str, Any]) -> Any:
        content = str(input_data)
        detection_method = config.get("detection_method", "comprehensive")
        confidence_threshold = config.get("confidence_threshold", 0.7)
        
        # ë‹¤ì¤‘ ë°©ë²•ìœ¼ë¡œ í™˜ê° íƒì§€
        detection_results = await self._multi_method_detection(content, detection_method)
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§
        filtered_results = self._filter_by_confidence(detection_results, confidence_threshold)
        
        return {
            "content_analyzed": content,
            "detection_method": detection_method,
            "hallucinations_detected": len(filtered_results) > 0,
            "detected_items": filtered_results,
            "overall_confidence": self._calculate_overall_confidence(detection_results),
            "recommendation": "reject" if filtered_results else "accept"
        }
    
    async def _multi_method_detection(self, content: str, method: str) -> List[Dict[str, Any]]:
        """ë‹¤ì¤‘ ë°©ë²• í™˜ê° íƒì§€"""
        detection_results = []
        
        if method in ["comprehensive", "pattern_based"]:
            pattern_results = await self._pattern_based_detection(content)
            detection_results.extend(pattern_results)
        
        if method in ["comprehensive", "statistical"]:
            statistical_results = await self._statistical_detection(content)
            detection_results.extend(statistical_results)
        
        if method in ["comprehensive", "semantic"]:
            semantic_results = await self._semantic_detection(content)
            detection_results.extend(semantic_results)
        
        return detection_results
    
    async def _pattern_based_detection(self, content: str) -> List[Dict[str, Any]]:
        """íŒ¨í„´ ê¸°ë°˜ í™˜ê° íƒì§€"""
        suspicious_patterns = [
            {
                "pattern": r'ì—°êµ¬ì— ë”°ë¥´ë©´.*í•˜ì§€ë§Œ.*ì¶œì²˜.*ì—†ë‹¤',
                "type": HallucinationType.SOURCE_FABRICATION,
                "confidence": 0.8
            },
            {
                "pattern": r'\d{4}ë…„.*ì¼ì–´ë‚ .*ì˜ˆì •.*í–ˆë‹¤',
                "type": HallucinationType.TEMPORAL_INCONSISTENCY,
                "confidence": 0.9
            },
            {
                "pattern": r'ëª¨ë“ .*í•­ìƒ.*í•˜ì§€ë§Œ.*ë•Œë•Œë¡œ',
                "type": HallucinationType.LOGICAL_CONTRADICTION,
                "confidence": 0.7
            },
            {
                "pattern": r'100%.*í™•ì‹¤.*ì•„ë§ˆë„.*ê°€ëŠ¥ì„±',
                "type": HallucinationType.LOGICAL_CONTRADICTION,
                "confidence": 0.75
            }
        ]
        
        detections = []
        for pattern_info in suspicious_patterns:
            matches = re.finditer(pattern_info["pattern"], content)
            for match in matches:
                detections.append({
                    "type": pattern_info["type"],
                    "confidence": pattern_info["confidence"],
                    "location": match.span(),
                    "text": match.group(),
                    "method": "pattern_based"
                })
        
        return detections
    
    async def _statistical_detection(self, content: str) -> List[Dict[str, Any]]:
        """í†µê³„ì  í™˜ê° íƒì§€"""
        detections = []
        
        # ìˆ«ì ì¼ê´€ì„± ê²€ì‚¬
        numbers = re.findall(r'\d+(?:\.\d+)?', content)
        percentages = re.findall(r'(\d+(?:\.\d+)?)%', content)
        
        # í¼ì„¼íŠ¸ í•©ê³„ ê²€ì‚¬
        if len(percentages) >= 2:
            total = sum(float(p) for p in percentages)
            if total > 110:  # 110% ì´ìƒì´ë©´ ì˜ì‹¬
                detections.append({
                    "type": HallucinationType.STATISTICAL_ERROR,
                    "confidence": 0.8,
                    "details": f"í¼ì„¼íŠ¸ í•©ê³„ ì´ìƒ: {total}%",
                    "method": "statistical"
                })
        
        # ë¹„í˜„ì‹¤ì  ìˆ˜ì¹˜ ê²€ì‚¬
        for num_str in numbers:
            try:
                num = float(num_str)
                if num > 1000000000:  # 10ì–µ ì´ìƒì˜ ìˆ˜ì¹˜
                    context = self._get_number_context(content, num_str)
                    if not self._is_reasonable_large_number(context):
                        detections.append({
                            "type": HallucinationType.FACTUAL_ERROR,
                            "confidence": 0.6,
                            "details": f"ë¹„í˜„ì‹¤ì  ìˆ˜ì¹˜: {num_str}",
                            "method": "statistical"
                        })
            except ValueError:
                continue
        
        return detections
    
    async def _semantic_detection(self, content: str) -> List[Dict[str, Any]]:
        """ì˜ë¯¸ì  í™˜ê° íƒì§€"""
        detections = []
        
        # ì˜ë¯¸ì  ëª¨ìˆœ ê²€ì‚¬ (ê°„ë‹¨í•œ ë²„ì „)
        contradictory_pairs = [
            (["ì¦ê°€", "ëŠ˜ì–´", "ìƒìŠ¹"], ["ê°ì†Œ", "ì¤„ì–´", "í•˜ë½"]),
            (["ê°€ëŠ¥", "í•  ìˆ˜"], ["ë¶ˆê°€ëŠ¥", "í•  ìˆ˜ ì—†"]),
            (["ìˆë‹¤", "ì¡´ì¬"], ["ì—†ë‹¤", "ë¶€ì¬"]),
            (["í•­ìƒ", "ì–¸ì œë‚˜"], ["ì ˆëŒ€", "ê²°ì½”"])
        ]
        
        sentences = content.split('.')
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            for positive_words, negative_words in contradictory_pairs:
                has_positive = any(word in sentence_lower for word in positive_words)
                has_negative = any(word in sentence_lower for word in negative_words)
                
                if has_positive and has_negative:
                    detections.append({
                        "type": HallucinationType.LOGICAL_CONTRADICTION,
                        "confidence": 0.7,
                        "text": sentence.strip(),
                        "details": "ê°™ì€ ë¬¸ì¥ ë‚´ ì˜ë¯¸ì  ëª¨ìˆœ",
                        "method": "semantic"
                    })
        
        return detections
    
    def _get_number_context(self, content: str, number_str: str) -> str:
        """ìˆ«ì ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ"""
        index = content.find(number_str)
        if index == -1:
            return ""
        
        start = max(0, index - 50)
        end = min(len(content), index + len(number_str) + 50)
        
        return content[start:end]
    
    def _is_reasonable_large_number(self, context: str) -> bool:
        """í° ìˆ«ìê°€ í•©ë¦¬ì ì¸ì§€ íŒë‹¨"""
        reasonable_contexts = [
            "ì¸êµ¬", "ë‹¬ëŸ¬", "ì›", "ê°œìˆ˜", "ê±°ë¦¬", "ë©´ì ", "ë°ì´í„°", "ë°”ì´íŠ¸"
        ]
        
        return any(ctx in context.lower() for ctx in reasonable_contexts)
    
    def _filter_by_confidence(self, detections: List[Dict[str, Any]], threshold: float) -> List[Dict[str, Any]]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ í•„í„°ë§"""
        return [d for d in detections if d.get("confidence", 0.0) >= threshold]
    
    def _calculate_overall_confidence(self, detections: List[Dict[str, Any]]) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not detections:
            return 0.95  # í™˜ê°ì´ íƒì§€ë˜ì§€ ì•Šìœ¼ë©´ ë†’ì€ ì‹ ë¢°ë„
        
        # íƒì§€ëœ í™˜ê°ì˜ í‰ê·  ì‹ ë¢°ë„
        avg_confidence = sum(d.get("confidence", 0.0) for d in detections) / len(detections)
        
        # í™˜ê°ì´ íƒì§€ë ìˆ˜ë¡ ì „ì²´ ì½˜í…ì¸ ì˜ ì‹ ë¢°ë„ëŠ” ë‚®ì•„ì§
        return max(0.1, 1.0 - avg_confidence)


class QualityAssessmentCapability(AgentCapability):
    """í’ˆì§ˆ í‰ê°€ ëŠ¥ë ¥"""
    
    def get_capability_name(self) -> str:
        return "quality_assessment"
    
    def get_supported_formats(self) -> List[str]:
        return ["text", "structured_content", "reports", "responses"]
    
    async def execute(self, input_data: Any, config: Dict[str, Any]) -> Any:
        content = input_data
        assessment_criteria = config.get("criteria", list(EvaluationDimension))
        weights = config.get("weights", {})
        
        # ê° ê¸°ì¤€ë³„ í’ˆì§ˆ í‰ê°€
        quality_scores = await self._assess_quality_dimensions(content, assessment_criteria)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = self._calculate_weighted_score(quality_scores, weights)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        quality_grade = self._determine_quality_grade(weighted_score)
        
        return {
            "content_assessed": content,
            "dimension_scores": quality_scores,
            "weighted_overall_score": weighted_score,
            "quality_grade": quality_grade,
            "assessment_summary": self._generate_assessment_summary(quality_scores),
            "improvement_areas": self._identify_improvement_areas(quality_scores)
        }
    
    async def _assess_quality_dimensions(self, content: Any, criteria: List[EvaluationDimension]) -> Dict[str, float]:
        """í’ˆì§ˆ ì°¨ì›ë³„ í‰ê°€"""
        scores = {}
        content_str = str(content)
        
        for criterion in criteria:
            if criterion == EvaluationDimension.FACTUAL_ACCURACY:
                scores["factual_accuracy"] = await self._assess_factual_accuracy(content_str)
            elif criterion == EvaluationDimension.LOGICAL_CONSISTENCY:
                scores["logical_consistency"] = await self._assess_logical_consistency(content_str)
            elif criterion == EvaluationDimension.RELEVANCE:
                scores["relevance"] = await self._assess_relevance(content_str)
            elif criterion == EvaluationDimension.COMPLETENESS:
                scores["completeness"] = await self._assess_completeness(content_str)
            elif criterion == EvaluationDimension.CLARITY:
                scores["clarity"] = await self._assess_clarity(content_str)
            elif criterion == EvaluationDimension.OBJECTIVITY:
                scores["objectivity"] = await self._assess_objectivity(content_str)
        
        return scores
    
    async def _assess_factual_accuracy(self, content: str) -> float:
        """ì‚¬ì‹¤ ì •í™•ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ í‰ê°€
        accuracy_indicators = [
            (r'ì¶œì²˜[:\s]*\[.*\]', 0.1),  # ì¶œì²˜ ëª…ì‹œ
            (r'ì—°êµ¬.*ë”°ë¥´ë©´', 0.05),      # ì—°êµ¬ ì¸ìš©
            (r'\d{4}ë…„', 0.03),          # êµ¬ì²´ì  ë…„ë„
            (r'ì•½\s*\d+', 0.02),         # ê·¼ì‚¬ê°’ í‘œí˜„
        ]
        
        base_score = 0.7
        for pattern, bonus in accuracy_indicators:
            if re.search(pattern, content):
                base_score += bonus
        
        # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ í˜ë„í‹°
        suspicious_patterns = [
            r'í™•ì‹¤íˆ.*ì•„ë§ˆë„',
            r'ëª¨ë“ .*í•˜ì§€ë§Œ.*ì¼ë¶€',
            r'í•­ìƒ.*ë•Œë•Œë¡œ'
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, content):
                base_score -= 0.1
        
        return min(max(base_score, 0.0), 1.0)
    
    async def _assess_logical_consistency(self, content: str) -> float:
        """ë…¼ë¦¬ì  ì¼ê´€ì„± í‰ê°€"""
        consistency_score = 0.8  # ê¸°ë³¸ ì ìˆ˜
        
        # ë…¼ë¦¬ì  ì—°ê²°ì–´ ì¡´ì¬
        logical_connectors = ["ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ì™œëƒí•˜ë©´", "ê²°ê³¼ì ìœ¼ë¡œ", "ë°˜ë©´"]
        connector_count = sum(1 for connector in logical_connectors if connector in content)
        consistency_score += min(connector_count * 0.02, 0.1)
        
        # ëª¨ìˆœ íŒ¨í„´ ê°ì 
        contradictions = [
            (r'ëª¨ë“ ', r'ì¼ë¶€'),
            (r'í•­ìƒ', r'ë•Œë•Œë¡œ'),
            (r'ì ˆëŒ€.*ì•Šë‹¤', r'ê°€ëŠ¥.*ìˆë‹¤')
        ]
        
        for pos_pattern, neg_pattern in contradictions:
            if re.search(pos_pattern, content) and re.search(neg_pattern, content):
                consistency_score -= 0.15
        
        return min(max(consistency_score, 0.0), 1.0)
    
    async def _assess_relevance(self, content: str) -> float:
        """ê´€ë ¨ì„± í‰ê°€"""
        # í‚¤ì›Œë“œ ë°€ë„ ê¸°ë°˜ ê°„ë‹¨ í‰ê°€
        content_length = len(content.split())
        
        if content_length == 0:
            return 0.0
        
        # ê¸°ë³¸ ê´€ë ¨ì„± ì ìˆ˜
        relevance_score = 0.75
        
        # ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        if content_length < 50:
            relevance_score -= 0.1  # ë„ˆë¬´ ì§§ìœ¼ë©´ ë¶ˆì™„ì „í•  ê°€ëŠ¥ì„±
        elif content_length > 1000:
            relevance_score -= 0.05  # ë„ˆë¬´ ê¸¸ë©´ ê´€ë ¨ ì—†ëŠ” ë‚´ìš© í¬í•¨ ê°€ëŠ¥ì„±
        
        return min(max(relevance_score, 0.0), 1.0)
    
    async def _assess_completeness(self, content: str) -> float:
        """ì™„ì „ì„± í‰ê°€"""
        completeness_indicators = [
            "ê²°ë¡ ",
            "ìš”ì•½",
            "ì •ë¦¬í•˜ë©´",
            "ë§ˆì§€ë§‰ìœ¼ë¡œ",
            "ì¢…í•©í•˜ë©´"
        ]
        
        structure_indicators = [
            "ì²«ì§¸",
            "ë‘˜ì§¸",
            "ë‹¤ìŒìœ¼ë¡œ",
            "ë˜í•œ",
            "ì¶”ê°€ë¡œ"
        ]
        
        base_score = 0.6
        
        # ê²°ë¡  ì¡´ì¬ì„±
        if any(indicator in content for indicator in completeness_indicators):
            base_score += 0.15
        
        # êµ¬ì¡°ì  ì™„ì„±ë„
        structure_count = sum(1 for indicator in structure_indicators if indicator in content)
        base_score += min(structure_count * 0.05, 0.2)
        
        # ê¸¸ì´ ê¸°ë°˜ ì™„ì„±ë„
        content_length = len(content.split())
        if content_length >= 100:
            base_score += 0.05
        
        return min(max(base_score, 0.0), 1.0)
    
    async def _assess_clarity(self, content: str) -> float:
        """ëª…í™•ì„± í‰ê°€"""
        sentences = [s.strip() for s in content.split('.') if s.strip()]
        
        if not sentences:
            return 0.0
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        clarity_score = 0.7
        
        # ìµœì  ë¬¸ì¥ ê¸¸ì´ (15-25 ë‹¨ì–´)
        if 15 <= avg_sentence_length <= 25:
            clarity_score += 0.15
        elif 10 <= avg_sentence_length <= 30:
            clarity_score += 0.1
        else:
            clarity_score -= 0.1
        
        # ë³µì¡í•œ ë‹¨ì–´ ë¹„ìœ¨
        all_words = content.split()
        complex_words = [word for word in all_words if len(word) > 8]
        complexity_ratio = len(complex_words) / max(len(all_words), 1)
        
        if complexity_ratio < 0.2:
            clarity_score += 0.1
        elif complexity_ratio > 0.4:
            clarity_score -= 0.15
        
        return min(max(clarity_score, 0.0), 1.0)
    
    async def _assess_objectivity(self, content: str) -> float:
        """ê°ê´€ì„± í‰ê°€"""
        subjective_indicators = [
            "ìƒê°í•©ë‹ˆë‹¤",
            "ëŠë‚ë‹ˆë‹¤",
            "ê°œì¸ì ìœ¼ë¡œ",
            "ì œ ì˜ê²¬",
            "ë¯¿ìŠµë‹ˆë‹¤",
            "ì¶”ì¸¡"
        ]
        
        objective_indicators = [
            "ì—°êµ¬ì— ë”°ë¥´ë©´",
            "ë°ì´í„°ì— ì˜í•˜ë©´",
            "í†µê³„ì ìœ¼ë¡œ",
            "ì¡°ì‚¬ ê²°ê³¼",
            "ë³´ê³ ì„œì—ì„œ"
        ]
        
        subjective_count = sum(1 for indicator in subjective_indicators if indicator in content)
        objective_count = sum(1 for indicator in objective_indicators if indicator in content)
        
        total_sentences = len([s for s in content.split('.') if s.strip()])
        
        objectivity_score = 0.8
        
        # ì£¼ê´€ì  í‘œí˜„ í˜ë„í‹°
        if total_sentences > 0:
            subjective_ratio = subjective_count / total_sentences
            objectivity_score -= subjective_ratio * 0.3
        
        # ê°ê´€ì  í‘œí˜„ ë³´ë„ˆìŠ¤
        if objective_count > 0:
            objectivity_score += min(objective_count * 0.05, 0.15)
        
        return min(max(objectivity_score, 0.0), 1.0)
    
    def _calculate_weighted_score(self, scores: Dict[str, float], weights: Dict[str, float]) -> float:
        """ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°"""
        if not weights:
            # ë™ì¼ ê°€ì¤‘ì¹˜
            return sum(scores.values()) / len(scores) if scores else 0.0
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for dimension, score in scores.items():
            weight = weights.get(dimension, 1.0)
            weighted_sum += score * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _determine_quality_grade(self, score: float) -> str:
        """í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if score >= 0.95:
            return "A+"
        elif score >= 0.9:
            return "A"
        elif score >= 0.85:
            return "A-"
        elif score >= 0.8:
            return "B+"
        elif score >= 0.75:
            return "B"
        elif score >= 0.7:
            return "B-"
        elif score >= 0.6:
            return "C"
        else:
            return "D"
    
    def _generate_assessment_summary(self, scores: Dict[str, float]) -> str:
        """í‰ê°€ ìš”ì•½ ìƒì„±"""
        best_aspect = max(scores, key=scores.get) if scores else "ì—†ìŒ"
        worst_aspect = min(scores, key=scores.get) if scores else "ì—†ìŒ"
        
        return f"ìµœê³  ì˜ì—­: {best_aspect} ({scores.get(best_aspect, 0):.2f}), ê°œì„  í•„ìš” ì˜ì—­: {worst_aspect} ({scores.get(worst_aspect, 0):.2f})"
    
    def _identify_improvement_areas(self, scores: Dict[str, float]) -> List[str]:
        """ê°œì„  ì˜ì—­ ì‹ë³„"""
        improvement_areas = []
        
        for dimension, score in scores.items():
            if score < 0.7:
                improvement_areas.append(dimension)
        
        return improvement_areas


class GuardrailEnforcementCapability(AgentCapability):
    """ê°€ë“œë ˆì¼ ê°•ì œ ëŠ¥ë ¥"""
    
    def get_capability_name(self) -> str:
        return "guardrail_enforcement"
    
    def get_supported_formats(self) -> List[str]:
        return ["text", "content", "responses", "user_inputs"]
    
    async def execute(self, input_data: Any, config: Dict[str, Any]) -> Any:
        content = str(input_data)
        guardrail_rules = config.get("rules", ["safety", "privacy", "bias", "harm"])
        enforcement_level = config.get("enforcement_level", "strict")
        
        # ê° ê°€ë“œë ˆì¼ ê·œì¹™ ê²€ì‚¬
        violations = await self._check_all_guardrails(content, guardrail_rules)
        
        # ê°•ì œ ìˆ˜ì¤€ì— ë”°ë¥¸ ì²˜ë¦¬
        enforcement_action = self._determine_enforcement_action(violations, enforcement_level)
        
        return {
            "content_checked": content,
            "guardrails_applied": guardrail_rules,
            "violations_found": violations,
            "enforcement_level": enforcement_level,
            "enforcement_action": enforcement_action,
            "safe_to_proceed": len(violations) == 0,
            "filtered_content": self._apply_content_filtering(content, violations) if violations else content
        }
    
    async def _check_all_guardrails(self, content: str, rules: List[str]) -> List[Dict[str, Any]]:
        """ëª¨ë“  ê°€ë“œë ˆì¼ ê·œì¹™ ê²€ì‚¬"""
        violations = []
        
        for rule in rules:
            if rule == "safety":
                safety_violations = await self._check_safety_guardrails(content)
                violations.extend(safety_violations)
            elif rule == "privacy":
                privacy_violations = await self._check_privacy_guardrails(content)
                violations.extend(privacy_violations)
            elif rule == "bias":
                bias_violations = await self._check_bias_guardrails(content)
                violations.extend(bias_violations)
            elif rule == "harm":
                harm_violations = await self._check_harm_guardrails(content)
                violations.extend(harm_violations)
        
        return violations
    
    async def _check_safety_guardrails(self, content: str) -> List[Dict[str, Any]]:
        """ì•ˆì „ì„± ê°€ë“œë ˆì¼ ê²€ì‚¬"""
        violations = []
        
        safety_patterns = [
            {
                "pattern": r'(í­ë ¥|ì‚´í•´|ê³µê²©|í…ŒëŸ¬)',
                "severity": "high",
                "description": "í­ë ¥ì  ë‚´ìš©"
            },
            {
                "pattern": r'(ìì‚´|ìí•´|ìƒì²˜)',
                "severity": "high", 
                "description": "ìí•´ ê´€ë ¨ ë‚´ìš©"
            },
            {
                "pattern": r'(ë¶ˆë²•|ë²”ì£„|ìœ„ë²•)',
                "severity": "medium",
                "description": "ë¶ˆë²• í™œë™ ì–¸ê¸‰"
            }
        ]
        
        for pattern_info in safety_patterns:
            if re.search(pattern_info["pattern"], content):
                violations.append({
                    "rule": "safety",
                    "type": pattern_info["description"],
                    "severity": pattern_info["severity"],
                    "pattern": pattern_info["pattern"],
                    "confidence": 0.8
                })
        
        return violations
    
    async def _check_privacy_guardrails(self, content: str) -> List[Dict[str, Any]]:
        """ê°œì¸ì •ë³´ ë³´í˜¸ ê°€ë“œë ˆì¼ ê²€ì‚¬"""
        violations = []
        
        privacy_patterns = [
            {
                "pattern": r'\d{3}-\d{4}-\d{4}',
                "type": "ì „í™”ë²ˆí˜¸",
                "severity": "high"
            },
            {
                "pattern": r'\d{6}-[1-4]\d{6}',
                "type": "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸",
                "severity": "high"
            },
            {
                "pattern": r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
                "type": "ì´ë©”ì¼ ì£¼ì†Œ",
                "severity": "medium"
            },
            {
                "pattern": r'ì‹ ìš©ì¹´ë“œ.*\d{4}.*\d{4}.*\d{4}.*\d{4}',
                "type": "ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸",
                "severity": "high"
            }
        ]
        
        for pattern_info in privacy_patterns:
            matches = re.finditer(pattern_info["pattern"], content)
            for match in matches:
                violations.append({
                    "rule": "privacy",
                    "type": pattern_info["type"],
                    "severity": pattern_info["severity"],
                    "location": match.span(),
                    "matched_text": match.group(),
                    "confidence": 0.9
                })
        
        return violations
    
    async def _check_bias_guardrails(self, content: str) -> List[Dict[str, Any]]:
        """í¸í–¥ ê°€ë“œë ˆì¼ ê²€ì‚¬"""
        violations = []
        
        bias_indicators = [
            {
                "patterns": [r'ëª¨ë“ \s+(ë‚¨ì|ì—¬ì)', r'(ë‚¨ì|ì—¬ì)ë“¤ì€\s+ë‹¤'],
                "type": "ì„±ë³„ í¸í–¥",
                "severity": "medium"
            },
            {
                "patterns": [r'(íŠ¹ì •êµ­ê°€).*ëª¨ë‘', r'(íŠ¹ì •ë¯¼ì¡±).*í•­ìƒ'],
                "type": "ì¸ì¢…/ë¯¼ì¡± í¸í–¥",
                "severity": "high"
            },
            {
                "patterns": [r'(ì Šì€ì´|ë…¸ì¸).*ë‹¤\s+ê·¸ë ‡ë‹¤', r'ì„¸ëŒ€.*íŠ¹ì§•.*ëª¨ë‘'],
                "type": "ì—°ë ¹ í¸í–¥",
                "severity": "medium"
            }
        ]
        
        for bias_info in bias_indicators:
            for pattern in bias_info["patterns"]:
                if re.search(pattern, content):
                    violations.append({
                        "rule": "bias",
                        "type": bias_info["type"],
                        "severity": bias_info["severity"],
                        "pattern": pattern,
                        "confidence": 0.7
                    })
        
        return violations
    
    async def _check_harm_guardrails(self, content: str) -> List[Dict[str, Any]]:
        """ìœ í•´ì„± ê°€ë“œë ˆì¼ ê²€ì‚¬"""
        violations = []
        
        harm_categories = [
            {
                "keywords": ["í˜ì˜¤", "ì°¨ë³„", "ë°°ì œ", "ë©¸ì‹œ"],
                "type": "í˜ì˜¤ í‘œí˜„",
                "severity": "high"
            },
            {
                "keywords": ["ê±°ì§“ë§", "ì¡°ì‘", "í—ˆìœ„", "ê°€ì§œ"],
                "type": "í—ˆìœ„ì •ë³´",
                "severity": "medium"
            },
            {
                "keywords": ["ìœ„í—˜í•œ", "í•´ë¡œìš´", "ë…ì„±", "ì¤‘ë…"],
                "type": "ìœ„í—˜ ì •ë³´",
                "severity": "high"
            }
        ]
        
        for harm_info in harm_categories:
            for keyword in harm_info["keywords"]:
                if keyword in content:
                    violations.append({
                        "rule": "harm",
                        "type": harm_info["type"],
                        "severity": harm_info["severity"],
                        "keyword": keyword,
                        "confidence": 0.6
                    })
        
        return violations
    
    def _determine_enforcement_action(self, violations: List[Dict[str, Any]], level: str) -> str:
        """ê°•ì œ ì¡°ì¹˜ ê²°ì •"""
        if not violations:
            return "allow"
        
        high_severity_count = sum(1 for v in violations if v.get("severity") == "high")
        medium_severity_count = sum(1 for v in violations if v.get("severity") == "medium")
        
        if level == "strict":
            if high_severity_count > 0:
                return "block"
            elif medium_severity_count > 0:
                return "filter"
            else:
                return "warn"
        elif level == "moderate":
            if high_severity_count >= 2:
                return "block"
            elif high_severity_count > 0 or medium_severity_count >= 3:
                return "filter"
            else:
                return "warn"
        else:  # lenient
            if high_severity_count >= 3:
                return "block"
            elif high_severity_count >= 2:
                return "filter"
            else:
                return "allow_with_warning"
    
    def _apply_content_filtering(self, content: str, violations: List[Dict[str, Any]]) -> str:
        """ì½˜í…ì¸  í•„í„°ë§ ì ìš©"""
        filtered_content = content
        
        for violation in violations:
            if "matched_text" in violation:
                # ë§¤ì¹­ëœ í…ìŠ¤íŠ¸ë¥¼ ë§ˆìŠ¤í‚¹
                matched_text = violation["matched_text"]
                mask = "*" * len(matched_text)
                filtered_content = filtered_content.replace(matched_text, mask)
            elif "keyword" in violation:
                # í‚¤ì›Œë“œë¥¼ ë§ˆìŠ¤í‚¹
                keyword = violation["keyword"]
                mask = "*" * len(keyword)
                filtered_content = filtered_content.replace(keyword, mask)
        
        return filtered_content


class EvaluatorAgent(ModularAgent):
    """LangChain ê¸°ë°˜ ì‘ë‹µ í‰ê°€ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: AgentConfig):
        # í”„ë ˆì„ì›Œí¬ ê²€ì¦
        if config.framework != AgentFramework.LANGCHAIN:
            raise ValueError(f"EvaluatorAgentëŠ” LangChain í”„ë ˆì„ì›Œí¬ë§Œ ì§€ì›í•©ë‹ˆë‹¤. í˜„ì¬: {config.framework}")
        
        super().__init__(config)
        
        # í‰ê°€ ì„¤ì •
        self.quality_thresholds = config.custom_config.get("quality_thresholds", {
            'accuracy': 0.8,
            'completeness': 0.7,
            'relevance': 0.75,
            'safety': 0.9
        })
        self.guardrail_enforcement_level = config.custom_config.get("enforcement_level", "strict")
        
    async def _register_default_capabilities(self):
        """ê¸°ë³¸ ëŠ¥ë ¥ ë“±ë¡"""
        # í™˜ê° íƒì§€ ëŠ¥ë ¥
        hallucination_capability = HallucinationDetectionCapability()
        self.capability_registry.register_capability(hallucination_capability)
        
        # í’ˆì§ˆ í‰ê°€ ëŠ¥ë ¥
        quality_capability = QualityAssessmentCapability()
        self.capability_registry.register_capability(quality_capability)
        
        # ê°€ë“œë ˆì¼ ê°•ì œ ëŠ¥ë ¥
        guardrail_capability = GuardrailEnforcementCapability()
        self.capability_registry.register_capability(guardrail_capability)
        
        self.logger.info("EvaluatorAgent ê¸°ë³¸ ëŠ¥ë ¥ ë“±ë¡ ì™„ë£Œ")
    
    async def _load_framework_adapter(self) -> FrameworkAdapter:
        """LangChain ì–´ëŒ‘í„° ë¡œë“œ"""
        adapter = LangChainAdapter()
        
        # ì—ì´ì „íŠ¸ë³„ ë§ì¶¤ ì„¤ì •
        framework_config = {
            "openai_api_key": self.config.custom_config.get("openai_api_key", "default-key"),
            "model_name": self.config.custom_config.get("model_name", "gpt-3.5-turbo"),
            "temperature": self.config.custom_config.get("temperature", 0.1),
            "max_tokens": self.config.custom_config.get("max_tokens", 1000),
            "verbose": self.config.custom_config.get("verbose", False)
        }
        
        await adapter.initialize(framework_config)
        return adapter
    
    async def comprehensive_evaluation(self, content: Any, evaluation_criteria: List[str]) -> Dict[str, Any]:
        """ì¢…í•©ì  í‰ê°€"""
        evaluation_request = ProcessingRequest(
            request_id=f"comprehensive_eval_{int(time.time())}",
            content=content,
            content_type="comprehensive_evaluation",
            processing_options={
                "evaluation_criteria": evaluation_criteria,
                "include_hallucination_check": True,
                "include_guardrail_check": True,
                "quality_thresholds": self.quality_thresholds
            }
        )
        
        response = await self.process(evaluation_request)
        
        return {
            "original_content": content,
            "evaluation_report": response.processed_content,
            "overall_confidence": response.confidence_score,
            "quality_metrics": response.quality_metrics,
            "evaluation_metadata": response.metadata,
            "recommendation": "approve" if response.confidence_score >= 0.8 else "revise"
        }
    
    async def detect_hallucinations_advanced(self, content: Any, reference_data: Optional[str] = None) -> Dict[str, Any]:
        """ê³ ê¸‰ í™˜ê° íƒì§€"""
        hallucination_capability = self.capability_registry.get_capability("hallucination_detection")
        
        if hallucination_capability:
            result = await hallucination_capability.execute(content, {
                "detection_method": "comprehensive",
                "confidence_threshold": 0.7,
                "reference_data": reference_data
            })
            
            return result
        else:
            return {
                "error": "í™˜ê° íƒì§€ ëŠ¥ë ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "hallucinations_detected": False
            }
    
    async def enforce_safety_guardrails(self, content: Any, rules: List[str]) -> Dict[str, Any]:
        """ì•ˆì „ ê°€ë“œë ˆì¼ ê°•ì œ"""
        guardrail_capability = self.capability_registry.get_capability("guardrail_enforcement")
        
        if guardrail_capability:
            result = await guardrail_capability.execute(content, {
                "rules": rules,
                "enforcement_level": self.guardrail_enforcement_level
            })
            
            return result
        else:
            return {
                "error": "ê°€ë“œë ˆì¼ ê°•ì œ ëŠ¥ë ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                "safe_to_proceed": True
            }


# ì‚¬ìš© ì˜ˆì‹œ
async def example_evaluator_usage():
    """EvaluatorAgent ì‚¬ìš© ì˜ˆì‹œ"""
    
    # ì„¤ì • ìƒì„±
    config = AgentConfig(
        agent_id="evaluator_main",
        framework=AgentFramework.LANGCHAIN,
        capabilities=["hallucination_detection", "quality_assessment", "guardrail_enforcement"],
        processing_mode=ProcessingMode.ASYNC,
        custom_config={
            "quality_thresholds": {
                "accuracy": 0.85,
                "completeness": 0.8,
                "relevance": 0.8,
                "safety": 0.95
            },
            "enforcement_level": "strict",
            "openai_api_key": "your-api-key",
            "model_name": "gpt-3.5-turbo"
        }
    )
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    evaluator = EvaluatorAgent(config)
    
    # í‰ê°€í•  ìƒ˜í”Œ ì½˜í…ì¸ 
    sample_content = """
    Pythonì€ 1991ë…„ ê·€ë„ ë°˜ ë¡œì„¬ì´ ê°œë°œí•œ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.
    ê°„ë‹¨í•˜ê³  ì½ê¸° ì‰¬ìš´ êµ¬ë¬¸ì„ ê°€ì§€ê³  ìˆì–´ ì´ˆë³´ìì—ê²Œ ì¸ê¸°ê°€ ë†’ìŠµë‹ˆë‹¤.
    ì›¹ ê°œë°œ, ë°ì´í„° ë¶„ì„, ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.
    Djangoì™€ FlaskëŠ” ëŒ€í‘œì ì¸ Python ì›¹ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    """
    
    # ê¸°ë³¸ í‰ê°€ ìš”ì²­
    request = ProcessingRequest(
        request_id="test_evaluation_001",
        content=sample_content,
        content_type="content_evaluation",
        processing_options={
            "evaluation_criteria": ["factual_accuracy", "logical_consistency", "completeness"],
            "original_query": "Python í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
        }
    )
    
    # í‰ê°€ ì‹¤í–‰
    response = await evaluator.process(request)
    
    print(f"í‰ê°€ ì™„ë£Œ: ì‹ ë¢°ë„ {response.confidence_score:.2f}")
    print(f"ì‚¬ìš©ëœ í”„ë ˆì„ì›Œí¬: {response.framework_info['name']}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {response.processing_time:.2f}ì´ˆ")
    
    # ì¢…í•©ì  í‰ê°€ í…ŒìŠ¤íŠ¸
    comprehensive_result = await evaluator.comprehensive_evaluation(
        sample_content,
        ["factual_accuracy", "logical_consistency", "relevance", "safety"]
    )
    
    print(f"ì¢…í•© í‰ê°€ ì™„ë£Œ: {comprehensive_result['recommendation']}")
    print(f"ì „ì²´ ì‹ ë¢°ë„: {comprehensive_result['overall_confidence']:.2f}")
    
    # í™˜ê° íƒì§€ í…ŒìŠ¤íŠ¸
    hallucination_result = await evaluator.detect_hallucinations_advanced(
        "Pythonì€ 2025ë…„ì— ìƒˆë¡œ ê°œë°œëœ ì–¸ì–´ì…ë‹ˆë‹¤."  # ì˜ë„ì  ì˜ëª»ëœ ì •ë³´
    )
    
    print(f"í™˜ê° íƒì§€: {hallucination_result.get('hallucinations_detected', False)}")
    
    # ê°€ë“œë ˆì¼ í…ŒìŠ¤íŠ¸
    safety_result = await evaluator.enforce_safety_guardrails(
        sample_content,
        ["safety", "privacy", "bias"]
    )
    
    print(f"ì•ˆì „ì„± ê²€ì‚¬: {safety_result.get('safe_to_proceed', True)}")
    
    return response


if __name__ == "__main__":
    asyncio.run(example_evaluator_usage())