import asyncio
from datetime import datetime
import aiohttp
from typing import Dict, Any, Optional
import logging
import time

from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from control.modular_agent_architecture import FrameworkAdapter, ProcessingRequest, ProcessingResponse

from ..core.cache import SearchCache
from ..models.state import AgentState
from ..models.data_models import RetrievalTask, SearchMetrics
from ..models.enums import DataSource
from ..tools.web_search import EnhancedWebSearchTool
from ..tools.api_retriever import APIRetrieverTool
from ..tools.multimedia import MultimediaProcessorTool
from ..tools.evaluator import RelevanceEvaluatorTool
from ..workflows.nodes import WorkflowNodes
from ..utils.exceptions import handle_langchain_exceptions
from ..utils.metrics import MetricsCalculator

# MCP í´ë¼ì´ì–¸íŠ¸ import
from ..mcp_client import MCPClient, DEFAULT_MCP_SERVERS

class LangChainAdapter(FrameworkAdapter):
    """LangChain í”„ë ˆì„ì›Œí¬ ì–´ëŒ‘í„° - MCP í†µí•© ì™„ì „ ê°œì„ ëœ ë²„ì „"""

    def __init__(self):
        self.workflow: Optional[StateGraph] = None
        self.agents: Dict[str, AgentExecutor] = {}
        self.tools: Dict[str, Any] = {}
        self.llm = None
        self.is_initialized = False
        self._search_cache = SearchCache()
        self._http_session: Optional[aiohttp.ClientSession] = None
        self._semaphore = asyncio.Semaphore(5)
        self.logger = logging.getLogger(f"{__name__}.LangChainAdapter")
        self.search_metrics = SearchMetrics()
        self.workflow_nodes: Optional[WorkflowNodes] = None
        self.metrics_calculator = MetricsCalculator()
        
        # MCP í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€
        self.mcp_client: Optional[MCPClient] = None

    def get_framework_name(self) -> str:
        return "LangChain"

    @handle_langchain_exceptions(fallback_value=False)
    async def initialize(self, config: Dict[str, Any]) -> bool:
        """LangChain ì´ˆê¸°í™” - MCP í†µí•© ë²„ì „"""
        if not self._http_session:
            timeout = aiohttp.ClientTimeout(total=config.get("request_timeout", 30))
            self._http_session = aiohttp.ClientSession(timeout=timeout)

        # LLM ì„¤ì •
        try:
            self.llm = ChatOpenAI(
                model=config.get("llm_model", "gpt-3.5-turbo"),
                temperature=config.get("temperature", 0.1),
                request_timeout=config.get("llm_timeout", 30)
            )
        except Exception as e:
            self.logger.warning(f"OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, í´ë°± ëª¨ë“œ ì‚¬ìš©")
            self.llm = None

        # MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        await self._initialize_mcp_client(config)

        # ë„êµ¬ ì´ˆê¸°í™” (MCP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
        await self._initialize_tools(config)

        # ì—ì´ì „íŠ¸ë“¤ ìƒì„±
        if self.llm:
            await self._create_specialist_agents(config)
            self._build_workflow()

        self.is_initialized = True
        self.logger.info("LangChain ì–´ëŒ‘í„° ì´ˆê¸°í™” ì™„ë£Œ (MCP í†µí•©)")
        return True

    async def _initialize_mcp_client(self, config: Dict[str, Any]):
        """MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # ì„¤ì •ì—ì„œ MCP ì„œë²„ êµ¬ì„± ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            mcp_servers = config.get("mcp_servers", DEFAULT_MCP_SERVERS)
            
            self.mcp_client = MCPClient(mcp_servers)
            success = await self.mcp_client.initialize()
            
            if success:
                self.logger.info("MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
                # ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ë¡œê·¸
                available_tools = await self.mcp_client.get_available_tools()
                self.logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ MCP ë„êµ¬: {available_tools}")
            else:
                self.logger.warning("MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨, í´ë°± ëª¨ë“œ ì‚¬ìš©")
                self.mcp_client = None
                
        except Exception as e:
            self.logger.error(f"MCP í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            self.mcp_client = None

    async def _initialize_tools(self, config: Dict[str, Any]):
        """ë„êµ¬ ì´ˆê¸°í™” - MCP í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"""
        if self.mcp_client:
            # MCP ê¸°ë°˜ ë„êµ¬ë“¤
            self.tools = {
                "web_search": EnhancedWebSearchTool(self.mcp_client),
                "api_retriever": APIRetrieverTool(self.mcp_client),
                "multimedia_processor": MultimediaProcessorTool(),
                "relevance_evaluator": RelevanceEvaluatorTool()
            }
            self.logger.info(f"MCP ê¸°ë°˜ ë„êµ¬ {len(self.tools)}ê°œ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            # í´ë°±: ê¸°ì¡´ ë°©ì‹ ë„êµ¬ë“¤
            self.tools = {
                "web_search": EnhancedWebSearchTool(None),  # í´ë°± ëª¨ë“œ
                "api_retriever": APIRetrieverTool(None),    # í´ë°± ëª¨ë“œ
                "multimedia_processor": MultimediaProcessorTool(),
                "relevance_evaluator": RelevanceEvaluatorTool()
            }
            self.logger.warning("í´ë°± ëª¨ë“œë¡œ ë„êµ¬ ì´ˆê¸°í™”")

    @handle_langchain_exceptions(fallback_value=None)
    async def _create_specialist_agents(self, config: Dict[str, Any]):
        """ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ë“¤ ìƒì„± - MCP ë„êµ¬ ì‚¬ìš©"""
        # ì›¹ ê²€ìƒ‰ ì „ë¬¸ê°€ (MCP ê¸°ë°˜)
        web_tools = [self.tools["web_search"], self.tools["relevance_evaluator"]]
        web_prompt = PromptTemplate(
            template="""ë‹¹ì‹ ì€ MCP ì„œë²„ë¥¼ í†µí•œ ì›¹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ MCP ì„œë²„ë¥¼ í†µí•´ ìµœì‹ ì˜ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
            input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
        )

        web_agent = create_react_agent(self.llm, web_tools, web_prompt)
        self.agents["web_specialist"] = AgentExecutor(
            agent=web_agent,
            tools=web_tools,
            verbose=config.get("verbose", False),
            return_intermediate_steps=True
        )

        # API ë°ì´í„° ì „ë¬¸ê°€ (MCP ê¸°ë°˜)
        api_tools = [self.tools["api_retriever"]]
        api_prompt = PromptTemplate(
            template="""ë‹¹ì‹ ì€ MCP ì„œë²„ë¥¼ í†µí•œ API ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
MCP ì„œë²„ë¥¼ í†µí•´ GitHub, Stack Overflow ë“±ì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
            input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
        )

        api_agent = create_react_agent(self.llm, api_tools, api_prompt)
        self.agents["api_specialist"] = AgentExecutor(
            agent=api_agent,
            tools=api_tools,
            verbose=config.get("verbose", False),
            return_intermediate_steps=True
        )

        # ë©€í‹°ë¯¸ë””ì–´ ì „ë¬¸ê°€ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        multimedia_tools = [self.tools["multimedia_processor"]]
        multimedia_prompt = PromptTemplate(
            template="""ë‹¹ì‹ ì€ ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ì˜¤ë””ì˜¤ ì½˜í…ì¸ ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
            input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
        )

        multimedia_agent = create_react_agent(self.llm, multimedia_tools, multimedia_prompt)
        self.agents["multimedia_specialist"] = AgentExecutor(
            agent=multimedia_agent,
            tools=multimedia_tools,
            verbose=config.get("verbose", False),
            return_intermediate_steps=True
        )

        # í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        quality_tools = [self.tools["relevance_evaluator"]]
        quality_prompt = PromptTemplate(
            template="""ë‹¹ì‹ ì€ ì •ë³´ í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìˆ˜ì§‘ëœ ì •ë³´ì˜ í’ˆì§ˆ, ê´€ë ¨ì„±, ì‹ ì„ ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}

ì¿¼ë¦¬: {input}

{agent_scratchpad}

ì‘ë‹µ:""",
            input_variables=["tools", "tool_names", "input", "agent_scratchpad"]
        )

        quality_agent = create_react_agent(self.llm, quality_tools, quality_prompt)
        self.agents["quality_specialist"] = AgentExecutor(
            agent=quality_agent,
            tools=quality_tools,
            verbose=config.get("verbose", False),
            return_intermediate_steps=True
        )

        self.logger.info(f"MCP ê¸°ë°˜ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ {len(self.agents)}ê°œ ìƒì„± ì™„ë£Œ")

    def _build_workflow(self):
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± - MCP ê¸°ë°˜"""
        if not self.agents:
            return

        self.workflow_nodes = WorkflowNodes(self.agents)
        workflow = StateGraph(AgentState)

        # ì›Œí¬í”Œë¡œìš°ì— ë…¸ë“œ ì¶”ê°€
        workflow.add_node("web_search", self.workflow_nodes.web_search_node)
        workflow.add_node("api_search", self.workflow_nodes.api_search_node)
        workflow.add_node("multimedia_process", self.workflow_nodes.multimedia_node)
        workflow.add_node("quality_evaluation", self.workflow_nodes.quality_node)

        # ì—£ì§€ ì •ì˜ (ìˆœì°¨ ì‹¤í–‰)
        workflow.add_edge("web_search", "api_search")
        workflow.add_edge("api_search", "multimedia_process")
        workflow.add_edge("multimedia_process", "quality_evaluation")
        workflow.add_edge("quality_evaluation", END)

        # ì‹œì‘ì  ì„¤ì •
        workflow.set_entry_point("web_search")

        # ì»´íŒŒì¼
        self.workflow = workflow.compile()
        self.logger.info("MCP ê¸°ë°˜ LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ")

    async def process_request(self, request: ProcessingRequest) -> ProcessingResponse:
        """ìš”ì²­ ì²˜ë¦¬ - MCP í†µí•© ë²„ì „"""
        if not self.is_initialized:
            await self.initialize({})

        # ìºì‹œ ì •ë¦¬
        self._search_cache.clear_expired()

        # RetrievalTask ìƒì„±
        task = self._create_retrieval_task(request)

        if self.workflow and self.llm:
            return await self._process_with_mcp_workflow(request, task)
        else:
            return await self._process_with_fallback(request, task)

    async def _process_with_mcp_workflow(self, request: ProcessingRequest, task: RetrievalTask) -> ProcessingResponse:
        """MCP ì›Œí¬í”Œë¡œìš°ë¥¼ ì‚¬ìš©í•œ ì²˜ë¦¬"""
        start_time = time.time()
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state: AgentState = {
                "messages": [HumanMessage(content=str(request.content))],
                "query": str(request.content),
                "results": {},
                "current_step": "web_search",
                "metadata": request.processing_options,
                "task_info": task.to_dict(),
                "processing_history": ["mcp_workflow_started"]
            }

            # MCP ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self.workflow.ainvoke(initial_state)

            # ê²°ê³¼ í†µí•©
            integrated_results = await self._integrate_retrieval_results(final_state["results"])

            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = await self._calculate_retrieval_confidence(final_state["results"])

            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.search_metrics.total_searches += 1

            return ProcessingResponse(
                request_id=request.request_id,
                processed_content=integrated_results,
                confidence_score=confidence,
                quality_metrics=await self._calculate_retrieval_quality_metrics(final_state["results"]),
                processing_time=time.time() - start_time,
                framework_info=self.get_framework_info(),
                metadata={
                    "steps_executed": len(final_state["results"]),
                    "framework_used": "LangChain + LangGraph + MCP",
                    "mcp_enabled": True,
                    "cache_stats": self._search_cache.get_stats()
                }
            )

        except Exception as e:
            self.logger.error(f"MCP ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            self.search_metrics.failed_searches += 1
            return await self._process_with_fallback(request, task)

    def _create_retrieval_task(self, request: ProcessingRequest) -> RetrievalTask:
        """ìš”ì²­ì„ êµ¬ì¡°í™”ëœ ê²€ìƒ‰ ì‘ì—…ìœ¼ë¡œ ë³€í™˜"""
        return RetrievalTask(
            task_id=request.request_id,
            query=str(request.content),
            data_sources=[DataSource.WEB_SEARCH, DataSource.API_ENDPOINT, DataSource.MULTIMEDIA],
            priority=1,
            max_results=request.processing_options.get("max_results", 10),
            freshness_requirement=0.8,
            relevance_threshold=0.7
        )

    def get_framework_info(self) -> Dict[str, str]:
        """í”„ë ˆì„ì›Œí¬ ì •ë³´ - MCP í†µí•© ë²„ì „"""
        return {
            "name": "LangChain",
            "version": "1.0.0-mcp",
            "status": "active" if self.is_initialized else "initializing",
            "features": "mcp_integration,agent_orchestration,workflow_management,tool_integration,real_web_search,caching",
            "capabilities": f"agents:{len(self.agents)},tools:{len(self.tools)}",
            "mcp_enabled": "true" if self.mcp_client else "false",
            "cache_enabled": "true",
            "workflow_engine": "LangGraph" if self.workflow else "none"
        }

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - MCP í¬í•¨"""
        if self._http_session and not self._http_session.closed:
            await self._http_session.close()
        
        # MCP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
        if self.mcp_client:
            await self.mcp_client.cleanup()
            
        self.logger.info("LangChain ì–´ëŒ‘í„° ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ (MCP í¬í•¨)")

    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (ë³€ê²½ ì—†ìŒ)
    async def _integrate_retrieval_results(self, results: Dict[str, Any]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ í†µí•©"""
        integrated_content = "# ğŸ” ì¢…í•© ì •ë³´ ìˆ˜ì§‘ ê²°ê³¼ (MCP ê¸°ë°˜)\n\n"

        step_names = {
            "web_search": "MCP ì›¹ ê²€ìƒ‰",
            "api_search": "MCP API ë°ì´í„° ìˆ˜ì§‘",
            "multimedia": "ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„",
            "quality_evaluation": "í’ˆì§ˆ í‰ê°€"
        }

        for key, value in results.items():
            step_name = step_names.get(key, key)
            integrated_content += f"## ğŸ“‹ {step_name}\n"
            integrated_content += f"{value}\n\n"

        integrated_content += "---\n\n"

        # ìš”ì•½ ì •ë³´ ì¶”ê°€
        integrated_content += "## ğŸ“Š ìˆ˜ì§‘ ìš”ì•½\n"
        integrated_content += f"- **ì´ ìˆ˜ì§‘ ë‹¨ê³„**: {len(results)}ê°œ\n"
        integrated_content += f"- **ì²˜ë¦¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        integrated_content += f"- **ë°ì´í„° ì†ŒìŠ¤**: MCP ì›¹ ê²€ìƒ‰, MCP API, ë©€í‹°ë¯¸ë””ì–´, í’ˆì§ˆ í‰ê°€\n"
        integrated_content += f"- **í†µí•© ë°©ì‹**: MCP ì„œë²„ ê¸°ë°˜ ìˆœì°¨ ì²˜ë¦¬ ë° ê²°ê³¼ ë³‘í•©\n\n"

        # ì‹ ë¢°ë„ ì •ë³´
        avg_confidence = min(0.8 + (len(results) - 1) * 0.05, 0.95)
        integrated_content += f"## âœ… ì‹ ë¢°ë„ ì •ë³´\n"
        integrated_content += f"- **ì „ì²´ ì‹ ë¢°ë„**: {avg_confidence:.2f}/1.0\n"
        integrated_content += f"- **ì •ë³´ ì™„ì„±ë„**: {'ë†’ìŒ' if len(results) >= 3 else 'ë³´í†µ'}\n"
        integrated_content += f"- **MCP í†µí•©**: í™œì„±í™”ë¨\n"
        integrated_content += f"- **í™œìš© ê¶Œì¥ë„**: {'ì ê·¹ ê¶Œì¥' if avg_confidence >= 0.8 else 'ê²€í†  í›„ ì‚¬ìš©'}\n"

        return integrated_content

    async def _calculate_retrieval_confidence(self, results: Dict[str, Any]) -> float:
        """ê²€ìƒ‰ ì‹ ë¢°ë„ ê³„ì‚° - MCP ë³´ë„ˆìŠ¤ í¬í•¨"""
        if not results:
            return 0.0

        # ê¸°ë³¸ ì‹ ë¢°ë„ (ê²°ê³¼ ìˆ˜ì— ë”°ë¼)
        base_confidence = 0.6 + (len(results) / 10)

        # ì™„ì„±ë„ ë³´ë„ˆìŠ¤
        completeness_bonus = min(len(results) / 4, 1.0) * 0.2

        # ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        diversity_bonus = 0.1 if len(results) >= 3 else 0.05

        # MCP í†µí•© ë³´ë„ˆìŠ¤
        mcp_bonus = 0.05 if self.mcp_client else 0.0

        # ìµœì¢… ì‹ ë¢°ë„ ê³„ì‚°
        confidence = min(base_confidence + completeness_bonus + diversity_bonus + mcp_bonus, 1.0)

        return round(confidence, 2)

    async def _calculate_retrieval_quality_metrics(self, results: Dict[str, Any]) -> Dict[str, float]:
        """ê²€ìƒ‰ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚° - MCP í–¥ìƒ ë°˜ì˜"""
        if not results:
            return {"coverage": 0.0, "diversity": 0.0, "freshness": 0.0, "relevance": 0.0}

        # ì»¤ë²„ë¦¬ì§€: 4ê°œ ì „ë¬¸ê°€ ê¸°ì¤€
        coverage = min(len(results) / 4, 1.0)

        # ë‹¤ì–‘ì„±: ê²°ê³¼ ìˆ˜ì— ë”°ë¥¸ ë‹¤ì–‘ì„± (MCP ë³´ë„ˆìŠ¤)
        diversity_base = 0.9 if len(results) >= 4 else 0.8 if len(results) >= 3 else 0.7
        diversity = min(diversity_base + (0.05 if self.mcp_client else 0.0), 1.0)

        # ì‹ ì„ ë„: MCP ì„œë²„ ê¸°ë°˜ì´ë¯€ë¡œ ë†’ìŒ
        freshness = 0.98 if self.mcp_client else 0.95

        # ê´€ë ¨ì„±: MCP ì„œë²„ ê¸°ë°˜ í–¥ìƒ
        relevance = 0.90 if self.mcp_client else 0.85

        return {
            "coverage": round(coverage, 2),
            "diversity": round(diversity, 2),
            "freshness": round(freshness, 2),
            "relevance": round(relevance, 2),
            "overall": round((coverage + diversity + freshness + relevance) / 4, 2),
            "mcp_enhanced": self.mcp_client is not None
        }

    # ê¸°ì¡´ í´ë°± ë©”ì„œë“œë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€...
    async def _process_with_fallback(self, request: ProcessingRequest, task: RetrievalTask) -> ProcessingResponse:
        """í´ë°± ì²˜ë¦¬ ë°©ì‹ (MCP ì‹¤íŒ¨ ì‹œ)"""
        start_time = time.time()
        query = str(request.content)

        # ìºì‹œ í™•ì¸
        cached_result = self._search_cache.get(query)
        if cached_result:
            self.search_metrics.cache_hits += 1
            self.logger.info(f"ìºì‹œ íˆíŠ¸: {query}")
            return ProcessingResponse(
                request_id=request.request_id,
                processed_content=cached_result,
                confidence_score=0.8,
                quality_metrics={"cached": 1.0, "freshness": 0.9},
                processing_time=time.time() - start_time,
                framework_info=self.get_framework_info(),
                metadata={"source": "cache", "mcp_fallback": True}
            )

        # ìºì‹œ ë¯¸ìŠ¤ - í´ë°± ê²€ìƒ‰ ìˆ˜í–‰
        self.search_metrics.cache_misses += 1
        search_results = await self._perform_comprehensive_fallback_search(request)
        self._search_cache.set(query, search_results)

        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.search_metrics.total_searches += 1
        processing_time = time.time() - start_time

        return ProcessingResponse(
            request_id=request.request_id,
            processed_content=search_results,
            confidence_score=0.7,
            quality_metrics={"fallback": 1.0, "mcp_fallback": True},
            processing_time=processing_time,
            framework_info=self.get_framework_info(),
            metadata={
                "source": "fallback_search",
                "mcp_fallback": True,
                "search_metrics": self.search_metrics.__dict__
            }
        )

    async def _perform_comprehensive_fallback_search(self, request: ProcessingRequest) -> str:
        """í¬ê´„ì ì¸ í´ë°± ê²€ìƒ‰ - MCP ì‹¤íŒ¨ ì‹œ"""
        query = str(request.content)
        retrieval_mode = request.processing_options.get("retrieval_mode", "comprehensive")
        results = []

        # í´ë°± ì›¹ ê²€ìƒ‰ (MCP ì—†ì´)
        try:
            if self.tools["web_search"]:
                web_result = await self.tools["web_search"]._arun(query)
            else:
                web_result = "ì›¹ ê²€ìƒ‰ ë¶ˆê°€ (ë„êµ¬ ì—†ìŒ)"
            results.append(f"## í´ë°± ì›¹ ê²€ìƒ‰ ê²°ê³¼\n{web_result}")
        except Exception as e:
            self.logger.warning(f"í´ë°± ì›¹ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            results.append(f"## í´ë°± ì›¹ ê²€ìƒ‰ ê²°ê³¼\ní´ë°± ì›¹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # í´ë°± API ê²€ìƒ‰ (MCP ì—†ì´)
        try:
            if self.tools["api_retriever"]:
                api_result = await self.tools["api_retriever"]._arun(query)
            else:
                api_result = "API ê²€ìƒ‰ ë¶ˆê°€ (ë„êµ¬ ì—†ìŒ)"
            results.append(f"## í´ë°± API ê²€ìƒ‰ ê²°ê³¼\n{api_result}")
        except Exception as e:
            self.logger.warning(f"í´ë°± API ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            results.append(f"## í´ë°± API ê²€ìƒ‰ ê²°ê³¼\ní´ë°± API ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰ (ìš”ì²­ëœ ê²½ìš°)
        if retrieval_mode == "multimodal":
            multimedia_tool = MultimediaProcessorTool()
            multimedia_result = multimedia_tool._run(query)
            results.append(f"## ë©€í‹°ë¯¸ë””ì–´ ë¶„ì„ ê²°ê³¼\n{multimedia_result}")

        # í’ˆì§ˆ í‰ê°€
        quality_tool = RelevanceEvaluatorTool()
        quality_result = quality_tool._run("\n".join(results), query)
        results.append(f"## í’ˆì§ˆ í‰ê°€ ê²°ê³¼\n{quality_result}")

        # í†µí•© ê²°ê³¼ ìƒì„±
        integrated_content = f"""
# ì¢…í•© ì •ë³´ ìˆ˜ì§‘ ê²°ê³¼ (MCP í´ë°± ëª¨ë“œ)

**ê²€ìƒ‰ ì¿¼ë¦¬**: {query}
**ê²€ìƒ‰ ëª¨ë“œ**: {retrieval_mode}
**ìˆ˜ì§‘ ë‹¨ê³„**: {len(results)}ê°œ
**MCP ìƒíƒœ**: í´ë°± ëª¨ë“œ (MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨)

{chr(10).join(results)}

---

## ìš”ì•½

- **ì´ ì •ë³´ ì†ŒìŠ¤**: {len(results)}ê°œ
- **ê²€ìƒ‰ ë°©ì‹**: í´ë°± ê²€ìƒ‰ + êµ¬ì¡°í™”ëœ ë¶„ì„
- **ì‹ ë¢°ë„**: ì¤‘ê°„ (MCP í´ë°±)
- **ì²˜ë¦¬ ì‹œê°„**: {time.time():.2f}ì´ˆ

**ê¶Œì¥ì‚¬í•­**: MCP ì„œë²„ ì—°ê²°ì„ í™•ì¸í•˜ê³  ì¬ì‹œë„í•˜ì„¸ìš”. í˜„ì¬ ê²°ê³¼ëŠ” í´ë°± ëª¨ë“œë¡œ ì œê³µë©ë‹ˆë‹¤.
"""

        return integrated_content